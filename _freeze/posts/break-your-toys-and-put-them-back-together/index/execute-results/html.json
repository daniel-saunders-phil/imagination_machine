{
  "hash": "366bb32f5dedbfe223b4901eb030ec48",
  "result": {
    "markdown": "---\ntitle: Break your toys and glue them back together\nauthor: Daniel Saunders\ndate: '2023-09-13'\nbibliography: dissertation_bibliography.bib\ntoc: true\ntoc-depth: 2\nexecute:\n  echo: true\nimage: image.png\n---\n\nTo me, the most interesting question in the social sciences is what statistical practice will look like in 20 years. Criticisms of null hypothesis significance testing has become mainstream. Even if p-values still dominant the publication landscape, many of other approaches to analysis are making it into major journals. At the same time, the software that lets people build more customizable, interesting models has been getting faster and more accessible. Sometimes it feels like our biggest challenge is that we don't quite know what to do with all this power. As a community we need more examples of what another statistical practice might look like.\n\nI previously [wrote](https://daniel-saunders-phil.github.io/imagination_machine/posts/if-none-of-the-above-then-what/) about a case where I took a toy mechanistic model and fit it to the small dataset with 10 islands in Oceania. What's cool about this sort of analysis is that you don't need to substitute out your mechanistic model for a linear regression when performing data analysis. That sort of substitution can introduce all sorts of complications in reasoning from one model to another. The approach where you fit a mechanistic model is much clearer and, when the model inevitably fails, the failures take on a importance within the framework of the theory. \n\nThis post is a sequel. I'm going to continue working with that case study and show a small bit about how to iteratively tailor a toy mechanistic model to the messiness of the world.\n\n# Sometimes you just need to integrate\n\nThe demographic theory of technology is a toy model that relates population size to the advancement of technical skills. In many areas of social science, model-based thinking is valued for its ability to rigourly connect individual-level behavioural thinking to population outcomes. For full details, you'll need to read [part 1](https://daniel-saunders-phil.github.io/imagination_machine/posts/if-none-of-the-above-then-what/). To summarize though, we have two parameters, $a$ represents the inherit difficulty of learning a skill. This is shared by all individuals. $b$ represents the individual variability in learning - different people are better or worse at imitating. We have a population with $n$ people. The population-level process is a differential equation. This equation tells us how the average level of technical skill, denoted by $z'$, changes at each point in time.\n\n$$ z' = -a + b*(\\gamma + \\ln(n)) $$\n\nDifferential equations tell us how to get from one point in time to the next. But knowing how to step through time doesn't give us the full story. We need to be able to predict how many tools each island society should have, given their population size and a length of time. Each society might have been innovating for different lengths of time which means population size doesn't totally determine the outcome.\n\nFortunately, in this case, we can just integrate the equation with respect to time. One you specify $a,b,n$, the growth rate is a constant. It doesn't matter how many tools you already have, you'll grow by the same amount in the next period. To know the number of tools have 30 time-steps, we just multiply the growth rate by 30. The technical skill on an island at a certain point in time is just:\n\n$$ z = (-a + b*(\\gamma + \\ln(n)))*T $$\n\nIn the previous post, I suggested we'd need a differential equation solver to fit our model. That turned out to be erroneous. We can now fit our model much more quickly and the extra analytical rigour will allow us to derive a new model later down the road.\n\n## Fitting\n\nWe'll import the same data.\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pymc as pm\nimport numpy as np\nimport arviz as az\nimport networkx as nx\nimport pytensor.tensor as pt\nfrom scipy import stats\n\nseed = 1235\nrng = np.random.default_rng(seed)\n\ntry:\n    dk = pd.read_csv(\"Kline\",sep=\";\")\nexcept FileNotFoundError:\n    dk = pd.read_csv(\"https://raw.githubusercontent.com/pymc-devs/pymc-resources/main/Rethinking_2/Data/Kline\",sep=\";\")\n\ntimes = [3000,3000,3000,3500,3000,3350,600,3350,2845,800]\ndk['initial_settlement (BP)'] = times\ndk\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>culture</th>\n      <th>population</th>\n      <th>contact</th>\n      <th>total_tools</th>\n      <th>mean_TU</th>\n      <th>initial_settlement (BP)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Malekula</td>\n      <td>1100</td>\n      <td>low</td>\n      <td>13</td>\n      <td>3.2</td>\n      <td>3000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Tikopia</td>\n      <td>1500</td>\n      <td>low</td>\n      <td>22</td>\n      <td>4.7</td>\n      <td>3000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Santa Cruz</td>\n      <td>3600</td>\n      <td>low</td>\n      <td>24</td>\n      <td>4.0</td>\n      <td>3000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Yap</td>\n      <td>4791</td>\n      <td>high</td>\n      <td>43</td>\n      <td>5.0</td>\n      <td>3500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Lau Fiji</td>\n      <td>7400</td>\n      <td>high</td>\n      <td>33</td>\n      <td>5.0</td>\n      <td>3000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Trobriand</td>\n      <td>8000</td>\n      <td>high</td>\n      <td>19</td>\n      <td>4.0</td>\n      <td>3350</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Chuuk</td>\n      <td>9200</td>\n      <td>high</td>\n      <td>40</td>\n      <td>3.8</td>\n      <td>600</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Manus</td>\n      <td>13000</td>\n      <td>low</td>\n      <td>28</td>\n      <td>6.6</td>\n      <td>3350</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Tonga</td>\n      <td>17500</td>\n      <td>high</td>\n      <td>55</td>\n      <td>5.4</td>\n      <td>2845</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Hawaii</td>\n      <td>275000</td>\n      <td>low</td>\n      <td>71</td>\n      <td>6.6</td>\n      <td>800</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nRename our variables to T for time, N for population size, and tools for the total tools present in a society.\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=2}\n``` {.python .cell-code}\nT = dk['initial_settlement (BP)'].values\nN = dk.population.values\ntools = dk.total_tools.values\nlabel_id = dk.culture.values\n```\n:::\n\n\nThe model can be expressed compactly with:\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=3}\n``` {.python .cell-code}\nwith pm.Model() as m0:\n    \n    a = pm.Gamma('a',mu=0.05,sigma=0.1)\n    b = pm.Gamma('b',mu=0.005,sigma=0.01)\n    \n    mu = T * (-a + b*(np.euler_gamma + np.log(N)))\n\n    Y = pm.Poisson(\"Y\", mu=mu,observed=tools)\n```\n:::\n\n\nThe priors need some justication. First, the Gamma distributions have really small means. That is because the population and time values are quite large. If you push their values up a bit, you'd have explosive growth in technology. Second, I tuned the priors by looking at prior predictive simulations. I want the bulk of the prior predictions to be below 1000. Some of our islands only have about 1000 people. It is certainly possible that everyone on an island is an inventor but its unlikely. I simply took draws from the prior and kept adjusting the parameters until the growth rates were fairly stable and represented wide uncertainty.\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=4}\n``` {.python .cell-code}\npp = pm.draw(mu,draws=100)\npp\n\nshort_label = [i[:3] for i in label_id]\n\nfor val in pp[:,:]:\n    plt.plot(short_label,val,'o',alpha=0.3)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=595 height=411}\n:::\n:::\n\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=5}\n``` {.python .cell-code}\nwith m0:\n    trace0 = pm.sample(2000,target_accept=0.99,progressbar=False,idata_kwargs={\"log_likelihood\":True},random_seed=rng)\n\ntrace0.to_json(\"trace0\")\n```\n:::\n\n\nWe face a bit of challenge in fitting the model - the parameters $a$ and $b$ are collinear. Each represents a certain kind of difficulty and they can counterbalance each other. If $a$ is high, the skill is uniformly difficult to learn. But if $b$ is also high, it makes up for that because it provides more opportunities for novel innovation. So there are several combinations of $a$ and $b$ parameters that provide the same accuracy in predicting the oceania dataset. The figure below illustrates how the MCMC algorithm explores the parameter space. It travels back and forth on this narrow band of roughly equivalent posterior probability.\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=6}\n``` {.python .cell-code}\ntrace0 = az.from_json(\"trace0\")\naz.plot_pair(trace0);\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=641 height=441}\n:::\n:::\n\n\nCollinearity is a medium-sized problem. It will inflate the uncertainty on our parameters. If we really cared to pin down the value of $a$, we'd need extra information to first pin down the value of $b$, or visa-versa. Or, we should scrape this model and build a new one where there is only one difficulty parameter. In our case, I don't particularly care what value each parameter takes on. I'm more interested in whether our model can explain the available data.\n\nBelow I plot the predictions the model makes about island societies, given the estimated parameter values. I've plotted 50 random predictions to help represent the uncertainty propogating through the model. The model performs pretty well on 6 of the island societies, suggesting it could be an candidate explanation for dynamics of technical development in those societies. However, in Hawaii, Manus, Trobriand and Chuuk, the model doesn't have a good way to predict them. Hawaii and Chuuk both have more tools that they are supposed, given their development time and population size. Meanwhile, Manus and Trobriand have too few tools. \n\nAt this point, it doesn't look like the demographic theory of technology is very plausible. The worry persists even if you dealt with the collinearity problem. If you pushed the growth rate up a little bit, you'd over shoot Trobriand and Manus even more. If you pushed the growth rate down a little, the problem of Hawaii and Chuuk gets worse.\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=7}\n``` {.python .cell-code}\npost_pred_m0 = pm.sample_posterior_predictive(model=m0,trace=trace0,progressbar=False)\npredictions = post_pred_m0.posterior_predictive['Y'].values.reshape((8000,10))\n\nfig, ax = plt.subplots(figsize=(9,5))\n\nfor index in range(len(label_id)):\n    ax.text(np.log(N)[index], tools[index], label_id[index], size=12)\n\nfor k in range(50):\n    i = np.random.randint(low=0,high=4000)\n    plt.plot(np.log(N),predictions[i],'o',alpha=0.2,color=\"tab:blue\",markersize=5)\n    \nplt.plot(np.log(N),tools,'o',color=\"black\")\n    \nplt.xlabel(\"Log population size\");\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSampling: [Y]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-2.png){width=738 height=429}\n:::\n:::\n\n\nWhen it comes to fitting toy mechanistic models we have to be careful. They break more easily than linear regression. Frankly, I think that's a good thing. If your model fits every dataset, you cannot learn from it. However! If a toy model breaks for opaque reasons, then it is also hard to learn from. In our case, it's not initially easy to tell whether the demographic theory of technology is bunk or whether there are some simple features of the Oceanic societies that we have to add before the model works well.\n\nThe world is a complicated place. Many of those complications are case-specific - there are unique features of the history and geography of oceania that influence the distribution of technology. Toy models are usually developed to capture some core general features that should recur across a range of cases. Out-of-the-box they usually don't fit any cases well. \n\nThe way I envision a productive scientific workflow is this: first we fit the toy model. Then we slowly layer on extra features to tailor it to some specific domain. If the toy model actually does capture some core mechanism in the world, it should start to fit nicely after a bit of tailoring. If the fit of the model doesn't improve too much once we add the extra features, we should think there is something wrong with the core theory.\n\n# The Founder Effect\n\nThere is one thing left out of the model that feels inexcusable - people do not settle new islands empty handed. If a well-developed society in Fiji sends out explorers who stumble across Hawaii, the first settlers of Hawaii will bring with them the technical knowledge of Fiji. We can incorporate this information into the model by starting out each society from the technical level of their founder society.\n\nI did some reading on the expansion of humanity into Oceania (especially useful was @matisoo-smith2015) and pieced together a rough picture of how it happened. Below is a graph depicting the ancestry. Yellow societies were settled roughly 3500 years ago. Purple societies were settled within the last 1000 years. There is good deal of uncertainty in this map. For example, Fiji was likely settled by one of Santa Cruz, Tikopia or Malekula but we don't know which. Pinning down these uncertainities would certainly help the project but I'll leave them to the side at this point. As usual, my purpose is illustrate a style of statistical reasoning rather than decisively settle some scientific question.\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=8}\n``` {.python .cell-code}\nG = nx.DiGraph()\nisland_labels = np.concatenate((np.array([\"New Guinea\"]),dk.culture.values))\nG.add_nodes_from(island_labels)\nG.add_edges_from([(\"New Guinea\",\"Manus\"),\n                  (\"New Guinea\",\"Trobriand\"),\n                  (\"Trobriand\",\"Santa Cruz\"),\n                  (\"Trobriand\",\"Tikopia\"),\n                  (\"Trobriand\",\"Malekula\"),\n                  (\"Malekula\",\"Lau Fiji\"),\n                  (\"Lau Fiji\",\"Tonga\"),\n                  (\"Lau Fiji\",\"Chuuk\"),\n                  (\"Tonga\",\"Hawaii\")])\nnode_color = np.concatenate((np.array([3350]),T))\npos = {\"New Guinea\": [-0.6,-0.65],\n        \"Yap\":[-0.6,0.4],\n        \"Manus\":[-0.4,-0.4],\n        \"Trobriand\":[-0.35,-0.6],\n        \"Santa Cruz\":[0.05,-0.4],\n        \"Tikopia\":[0.15,-.55],\n        \"Malekula\":[0.2,-.7],\n        \"Lau Fiji\":[0.5,-0.7],\n        \"Tonga\":[0.7,-0.8],\n        \"Hawaii\":[0.7,0.8],\n        \"Chuuk\":[-0.1,0.4]}\nplt.figure(figsize=(9,6))\nnx.draw_networkx(G,node_color=node_color,cmap=\"plasma\",pos=pos,with_labels=True,node_size=2500,alpha=0.8)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=689 height=463}\n:::\n:::\n\n\nWe'll need a new model where the level of technical skill is dependent on two factors: the demographic model for the settled society plus the level of technology present in the founder society at the time of founding.  We'll need to break the development time into two periods. In the first period, the founder society is innovating with its population size. In the second period, the ancestor society settles on the new island and now that island starts innovating autonomously with its population size. The dividing line between the two periods is just the difference in their development time. \n\nFor example, suppose we want a function that tells us how many tools Hawaii should have. Hawaii was settled 800 years ago. Tonga was settled 2845 years ago. So we'd want to let Tonga innovate for 2045 years, record what ever skill level is present. Then we'd switch to Hawaii and innovate for 800 years using Hawaii's larger population size. Finally, we add the two skill levels together to get our final prediction. \n\nLet's generalize away from just Hawaii and Tonga to build an abstract model. Denote the founder society with subscript $f$ and the settled society with subscript $s$. The level of tools on the settled society by the time of observation is:\n\n$$ z_{s} = (-a + b*(\\gamma + \\ln(n_{s})))*T_{s} + (-a + b*(\\gamma + \\ln(n_{f})))*(T_{f} - T_{s})$$\n\nThe expression is long but it is just the original expression for the settled society, plus running the model for the founder society with an abbreviated length of time ($T_{f} - T_{s}$). Now we'll do a bunch of simplification and discover a surprisingly compact expression. \n\n## A surprisingly compact expression\n\nFirst, we'll expand the second term. That means multiply the original differential equation bit by $T_{f} - T_{s}$\n\n$$ (-a + b*(\\gamma + \\ln(n_{s})))*T_{s} + (-a + b*(\\gamma + \\ln(n_{f})))*T_{f} - (-a + b*(\\gamma + \\ln(n_{f})))*T_{s} $$\n\nNotice that the first and the last terms of the equations are almost the same - they have the core difference equation multiplied by $T_{s}$. The only difference is that one uses the population of the settled society, $\\ln(n_{s})$, and the other uses the population of the founding society, $\\ln(n_{f})$. It will turn out we can exploit this fact to perform a lot of simplification. What is left will show that we only need to care about the ratio of population sizes to incorporate founder effects. It will also mean that we can leave the middle term completely untouched. \n\nLet's distribute the time terms on the left and right sides of this expression.\n\n$$ (-aT_{s}  + bT_{s}(\\gamma + \\ln(n_{s})))+ (-a + b(\\gamma + \\ln(n_{f})))T_{f} - (-aT_{s} + bT_{s}(\\gamma + \\ln(n_{f}))) $$\n\nMultiply the third term by negative 1 to take care of the minus sign in front.\n\n$$ -aT_{s}  + bT_{s}(\\gamma + \\ln(n_{s})) + (-a + b(\\gamma + \\ln(n_{f})))T_{f} + aT_{s} - bT_{s}(\\gamma + \\ln(n_{f})) $$\n\nThe $-aT_{s}$ term in the front cancels out the $aT_{s}$ term toward the end.\n\n$$  bT_{s}(\\gamma + \\ln(n_{s})) + (-a + b(\\gamma + \\ln(n_{f})))T_{f} - bT_{s}(\\gamma + \\ln(n_{f}))$$\n\nDistribute the $bT_{s}$ terms at the front and back.\n\n$$  bT_{s}\\gamma + bT_{s}\\ln(n_{s}) + (-a + b*(\\gamma + \\ln(n_{f})))T_{f} - bT_{s}\\gamma - bT_{s}\\ln(n_{f})$$\n\nThe $bT_{s}\\gamma$'s cancel. \n\n$$  bT_{s}\\ln(n_{s}) + (-a + b*(\\gamma + \\ln(n_{f})))T_{f} - bT_{s}\\ln(n_{f})$$\n\nRearrange the expression to move the $bT_{s}\\ln(n_{f})$ term to the front.\n\n$$  bT_{s}\\ln(n_{s}) - bT_{s}\\ln(n_{f}) + (-a + b(\\gamma + \\ln(n_{f})))T_{f} $$\n\nFactor out $bT_{s}$ from the front.\n\n$$  bT_{s}(\\ln(n_{s}) - \\ln(n_{f})) + (-a + b(\\gamma + \\ln(n_{f})))T_{f} $$\n\nLastly, use the fact that the difference between logarithms is just the logarithm of the quotient.\n\n$$  bT_{s}(\\ln(n_{s} / n_{f})) + (-a + b(\\gamma + \\ln(n_{f})))T_{f} $$\n\nWe're done. We've arrived at what I call the founder model. The second half of the expression is the familiar expression for the founder society - it tells us what happens if we just let the founder society do all the innovating. The first half of the expression adjusts the original model based on how much time the settled society has been around and a ratio of their population sizes. When the settled society is bigger, the adjustment factor is positive. When the settled society is smaller, the adjustment factor is negative.\n\n## Fitting the founder model\n\nAppreciably, this model assumes that each island immediately takes on its mature population size upon being settled. That's silly. Population size is also a dynamic process. So some fairly important features of the world are left out. If you feel like that's too big of an omission, I can understand. At the very least it makes salient exactly why this sort of analysis cannot succeed without substantially more data and thinking. I care a lot more about understanding the techniques and capacities of mathematical modeling than actually answering the question of whether the demographic theory is any good.\n\nTo program it up, it's a bit tedious. We'll need a unique equation for each island and lot of indexing. Regardless, we get the luxury of keeping just two free parameters. \n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=9}\n``` {.python .cell-code}\nwith pm.Model() as m1:\n    # Specify prior distributions\n    a = pm.Gamma('a',mu=0.05,sigma=0.1)\n    b = pm.Gamma('b',mu=0.005,sigma=0.01)\n\n    y_yap = T[3] * (-a + b*(np.euler_gamma + np.log(N[3])))\n    y_manus = T[7] * (-a + b*(np.euler_gamma + np.log(N[7])))\n    y_trobriand = T[5] * (-a + b*(np.euler_gamma + np.log(N[5])))\n\n    # descendents of trobriand\n\n    y_santa_cruz = b*T[2]*pt.log(N[2] / N[5]) + y_trobriand\n    y_tikopia = b*T[1]*pt.log(N[1] / N[5]) + y_trobriand\n    y_malekula = b*T[0]*pt.log(N[0] / N[5]) + y_trobriand\n\n    # descendents of malekula\n\n    y_fiji = b*T[4]*pt.log(N[4] / N[0]) + y_malekula\n\n    # descendents of fiji\n\n    y_chuuk = b * T[6] * pt.log(N[6] / N[4]) + y_fiji\n    y_tonga = b * T[8] * pt.log(N[8] / N[4]) + y_fiji\n\n    # descendents of tonga\n\n    y_hawaii = b * T[9] * pt.log(N[9] / N[8]) + y_tonga\n\n    mu = pt.as_tensor([y_malekula,y_tikopia,y_santa_cruz,y_yap,y_fiji,y_trobriand,y_chuuk,y_manus,y_tonga,y_hawaii])\n\n    Y = pm.Poisson(\"Y\", mu=mu,observed=tools)\n```\n:::\n\n\nDespite the strange functional specification, the model fits as easily as the first.\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=10}\n``` {.python .cell-code}\nwith m1:\n    trace1 = pm.sample(2000,target_accept=0.99,progressbar=False,idata_kwargs={\"log_likelihood\":True},random_seed=rng)\ntrace1.to_json('trace1')\n```\n:::\n\n\nWe can now visualize the predictions. The model does a lot better explaining those islands that previously had too many tools. Chuuk is now squarely within the range of model uncertainties. Hawaii is closer. Manus and Trobriand are still a bit stubborn. This makes good sense - the structural changes we made meant that islands will, in general, have more tools than before. So if Trobriand and Manus previously had surprisingly few tools, we don't have many new modeling tricks to explain that.\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=11}\n``` {.python .cell-code}\ntrace1 = az.from_json(\"trace1\")\npost_pred_m1 = pm.sample_posterior_predictive(model=m1,trace=trace1,progressbar=False)\npredictions = post_pred_m1.posterior_predictive['Y'].values.reshape((8000,10))\n\nfig, ax = plt.subplots(figsize=(8,5))\n\nfor index in range(len(label_id)):\n    ax.text(np.log(N)[index], tools[index], label_id[index], size=12)\n\nfor k in range(50):\n    i = np.random.randint(low=0,high=4000)\n    plt.plot(np.log(N),predictions[i],'o',alpha=0.2,color=\"tab:blue\",markersize=5)\n    \nplt.plot(np.log(N),tools,'o',color=\"black\")\n    \nplt.xlabel(\"Log population size\");\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSampling: [Y]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-2.png){width=667 height=429}\n:::\n:::\n\n\nOur visual intuition that the founder model is doing better is validated by model comparison statistics.\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=12}\n``` {.python .cell-code}\naz.compare({\"Standard model\":trace0,\n            \"Founder model\":trace1})\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rank</th>\n      <th>elpd_loo</th>\n      <th>p_loo</th>\n      <th>elpd_diff</th>\n      <th>weight</th>\n      <th>se</th>\n      <th>dse</th>\n      <th>warning</th>\n      <th>scale</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Founder model</th>\n      <td>0</td>\n      <td>-45.805932</td>\n      <td>5.748673</td>\n      <td>0.000000</td>\n      <td>0.96632</td>\n      <td>6.236006</td>\n      <td>0.000000</td>\n      <td>True</td>\n      <td>log</td>\n    </tr>\n    <tr>\n      <th>Standard model</th>\n      <td>1</td>\n      <td>-115.594256</td>\n      <td>20.337151</td>\n      <td>69.788323</td>\n      <td>0.03368</td>\n      <td>34.748214</td>\n      <td>32.368356</td>\n      <td>True</td>\n      <td>log</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWhat I really like about this strategy of modeling is that we've improve the fit just by incorporating domain knowledge. Often, in statistical modeling, the techniques to improve fit involve making the model more flexible. With more adjustable parameters, a model will always fit better. But the improvement in fit doesn't mean we are discovering the true mechanical structure behind the data. It often only means our model is more flexible. But here we didn't increase the parametric flexibility. All we did was thought about the problem and did algebra. I think that's an underappreciated but highly powerful technique.\n\nI hope my position has become clear - toy models are easy to break but we should break them. Only by breaking them do we get our most informative analyses. When we slowly try to put them back together, we can often come up with clever structural adjustments to the model that improve fit without introducing inappropriate sorts of flexibility.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}