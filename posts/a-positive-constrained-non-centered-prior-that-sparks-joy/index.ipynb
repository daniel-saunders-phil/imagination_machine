{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: The trouble with positive-constrained hierarchical parameters\n",
        "---"
      ],
      "id": "0459581a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pytensor.tensor as pt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pymc as pm\n",
        "import arviz as az\n",
        "import pandas as pd\n",
        "from pymc_extras.prior import Prior"
      ],
      "id": "24155456",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. They can funnel just like hierarchical normals\n"
      ],
      "id": "98599a8f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with pm.Model() as m0:\n",
        "    sigma = pm.Gamma('sigma',mu=1.0,sigma=1.0)\n",
        "    phi = pm.LogNormal('phi',mu=1.0,sigma=sigma)\n",
        "    \n",
        "    t0 = pm.sample()"
      ],
      "id": "1789d83f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.plot_pair(t0,divergences=True,var_names=[\"phi\",\"sigma\"])\n",
        "plt.xlim(-1,10)\n",
        "plt.yscale(\"log\")"
      ],
      "id": "c8ece951",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Parameterizing in log space makes it hard to control the mean and variance of your distribution\n",
        "\n",
        "One strategy is to just work in log space. There, you can build a normal model that is unconstrained. This gives you access to familiar reparameterization techniques like non-centering. Then you exponential transform everything to enforce the positivity constraint.\n"
      ],
      "id": "4c177dd6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mu = 1\n",
        "\n",
        "with pm.Model() as m1:\n",
        "    sigma = pm.Gamma('sigma',mu=1,sigma=1)\n",
        "\n",
        "    # non-centered trick\n",
        "    log_phi_z = pm.Normal(\"phi_offset\",mu=0,sigma=1) \n",
        "    log_phi = mu + log_phi_z * sigma\n",
        "\n",
        "    # make it positive\n",
        "    phi = pm.Deterministic(\"phi\",pm.math.exp(log_phi))\n",
        "    \n",
        "    t1 = pm.sample()"
      ],
      "id": "cf4bf7df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.plot_pair(t1,divergences=True,var_names=[\"phi\",\"sigma\"])\n",
        "plt.xlim(-1,10)\n",
        "plt.yscale(\"log\")"
      ],
      "id": "f273ae3b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The problem is that we quickly lose control over the mean and variance. The exponential transform introduces a very long tail behaviour which blows the distribution up. Check out that mean and standard deviation:\n"
      ],
      "id": "2b93443e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(t1,var_names=[\"phi\"])"
      ],
      "id": "d4087c1f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The class\n",
        "\n",
        "One can easily get lost trying to guess how to generate the desired distribution in the positive space. A tempting line of thinking is that you can just take the log of your desired mean and variance and then pass those to the normal.\n",
        "\n",
        "Taking the logs or any number of other heuristic approaches runs into two main problems:\n",
        "1. Applying exponentiation will entangle a distribution's mean and variance. If the mean gets larger, so does the variance. If the variance gets larger, so does the mean. So your distribution will have some huge values in its typical set. If you run away from those huge values by reducing the mean and variance, you'll get a distribution tightly bunched around 0. This will probably be too strongly informative to recover the values you are interested in. \n",
        "2. Even if you can get the mean that you want, we aren't working with a single value here. We have a distribution of parameters. We'd also need to find a way to make sure our distribution is well-behaved for all parameters in the typical set of our hyperparameters. \n",
        "\n",
        "The modeler who works on heuristics is not prepared to handle the mystical transformative powers of exponentiation. [Wikipedia](https://en.wikipedia.org/wiki/Log-normal_distribution#Definitions), however, operates on the wisdom of generations and offers a path through the mists. To generate a log-normal distribution with our desired mean ($\\mu_{X}$) and variance ($\\sigma_{X}^2$), we have these formulae:\n",
        "\n",
        "$$\\mu = \\ln(\\frac{\\mu_{X}^2}{\\sqrt{\\mu_{X}^2 + \\sigma_{X}^2}})$$\n",
        "\n",
        "$$\\sigma^2 = \\ln(1 + \\frac{\\sigma_{X}^2}{\\mu_{X}^2})$$\n",
        "\n",
        "If we put these formulae inside a pymc model, we can make the appropriate correction for every combination of hyperparameters. Here's a class, in the style of the [Prior class](https://www.pymc.io/projects/extras/en/stable/generated/pymc_extras.prior.Prior.html), that implements this approach.\n"
      ],
      "id": "8b39db85"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pytensor.tensor import TensorVariable\n",
        "from pymc_extras.prior import create_dim_handler\n",
        "\n",
        "class LogNormalExp: \n",
        "    def __init__(self, dims: tuple | None = None, centered: bool = True, **parameters):\n",
        "        self.parameters = parameters\n",
        "        self.dims = dims\n",
        "        self.centered = centered\n",
        "\n",
        "    def _create_parameter(self, param, value, name):\n",
        "        if not hasattr(value, \"create_variable\"):\n",
        "            return value\n",
        "\n",
        "        child_name = f\"{name}_{param}\"\n",
        "        return self.dim_handler(value.create_variable(child_name), value.dims)\n",
        "    \n",
        "    def create_variable(self, name: str) -> TensorVariable: \n",
        "        self.dim_handler = create_dim_handler(self.dims)\n",
        "        parameters = {\n",
        "            param: self._create_parameter(param, value, name)\n",
        "            for param, value in self.parameters.items()\n",
        "        }\n",
        "        # transformation trick to constrain the mu and sigma\n",
        "        mu_log = pt.log(parameters['mu']**2 / pt.sqrt(parameters['mu']**2 + parameters['sigma']**2))\n",
        "        sigma_log = pt.sqrt(pt.log(1 + (parameters['sigma']**2 / parameters['mu']**2)))\n",
        "\n",
        "        if self.centered:\n",
        "            log_phi = pm.Normal(name+ \"_log\", mu=mu_log, sigma=sigma_log, dims=self.dims)\n",
        "\n",
        "        else:\n",
        "            log_phi_z = pm.Normal(name + \"_log\" + \"_offset\", mu=0, sigma=1, dims=self.dims) \n",
        "            log_phi = mu_log + log_phi_z * sigma_log\n",
        "\n",
        "        phi = pm.math.exp(log_phi)\n",
        "        phi = pm.Deterministic(name, phi, dims=self.dims)\n",
        "        \n",
        "        return phi\n",
        "\n",
        "LogNormalExp(mu=1, sigma=1);"
      ],
      "id": "29c0108a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Benefits of the class over alternatives\n",
        "\n",
        "I just want to explore one more approach that doesn't work because it makes it vivid just how effective this approach is. You might be tempted to take the desired mean and standard, push them through the formula, and then build your hyperparameters around the desired values. Suppose you want a hierarchical model where group-level parameters have a mean of 1.5 and a standard deviation of 0.5. One option is:\n"
      ],
      "id": "e3166f19"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "desired_mean = 1.5\n",
        "desired_std = 0.5\n",
        "\n",
        "mu_log = np.log(desired_mean**2 / np.sqrt(desired_mean**2 + desired_std**2))\n",
        "sigma_log = np.sqrt(np.log(1 + (desired_std**2 / desired_mean**2)))\n",
        "\n",
        "mu_log, sigma_log"
      ],
      "id": "2f639e8c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "coords ={\n",
        "    \"groups\": np.arange(5)\n",
        "}\n",
        "\n",
        "with pm.Model(coords=coords) as model_standard:\n",
        "    mu = pm.Gamma(\"mu\", mu=mu_log, sigma=0.25) # i'm centering around 0.35ish with a little bit of wiggle room\n",
        "    sigma = pm.Gamma(\"sigma\", mu=sigma_log, sigma=0.25) # same here for standard deviation\n",
        "    \n",
        "    log_phi = pm.Normal(\"log_phi\", mu=mu, sigma=sigma, dims=\"groups\")\n",
        "    phi = pm.Deterministic(\"phi\", pt.exp(log_phi))\n",
        "\n",
        "    prior = pm.sample_prior_predictive()\n",
        "\n",
        "az.plot_density(\n",
        "    prior.prior,\n",
        "    var_names=[\"phi\"],\n",
        "    shade=0.8\n",
        ")\n",
        "plt.ylim(-0.1,10);"
      ],
      "id": "4b827b27",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You'll notice that the means of these the prior samples are slightly above the desired value (1.5). Meanwhile, if we look at the summary, you'll notice that the standard deviations are much higher than the desired value. This arises we've only been able to partially apply the transformation we need. We took the expected value of the hyperparameters and ensured those work out. But we have no guarantee that values further away from the expected value will map to the correct place in log space. So we get an inflation of the variances.\n"
      ],
      "id": "e0f1fff0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(\n",
        "    prior.prior,\n",
        "    var_names=[\"phi\"],\n",
        ")"
      ],
      "id": "f424133a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By contrast, if we use the internal transformation, we apply the same transform to every value of the hyper-parameters, expected or not. You'll notice that the standard deviation and mean adhere very closely to our desired values.\n"
      ],
      "id": "4736c799"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "coords ={\n",
        "    \"groups\": np.arange(5)\n",
        "}\n",
        "\n",
        "with pm.Model(coords=coords) as model_special:\n",
        "    mu = pm.Gamma(\"mu\", mu=desired_mean, sigma=0.25)\n",
        "    sigma = pm.Gamma(\"sigma\", mu=desired_std, sigma=0.25)\n",
        "    phi = LogNormalExp(mu=mu, sigma=sigma, dims=\"groups\").create_variable(\"phi\")\n",
        "\n",
        "    prior = pm.sample_prior_predictive()\n",
        "\n",
        "az.plot_density(\n",
        "    prior.prior,\n",
        "    var_names=[\"phi\"],\n",
        "    shade=0.8\n",
        ");"
      ],
      "id": "a56535d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(\n",
        "    prior.prior,\n",
        "    var_names=[\"phi\"],\n",
        ")"
      ],
      "id": "22c3a100",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Application in MMMs\n",
        "\n",
        "This prior really shines in MMMs. They have positive constrained parameters (efficiency and saturation can both take on only positive values). They also really need non-centered parameterizations. The rule of thumb is that non-centered priors work better when the information in the data is limited. In real-world marketing data, we typically learn a lot about the saturation point (because the company is overspending on media) or we learn a lot about the efficiency (because the company is underspending on media) [but not both](https://daniel-saunders-phil.github.io/imagination_machine/posts/geometric-intuition-mmm/). So if we have hierarchical parameters on both types of parameters, at least one of them will want a non-centered prior. \n",
        "\n",
        "In the current (or forthcoming, depending on when you are reading this), hierarchical model in [pymc-marketing examples](https://www.pymc-marketing.io/en/stable/notebooks/mmm/mmm_multidimensional_example.html), we use the naive exponentiated non-centered normal distribution to solve that problem. \n",
        "\n",
        "The example looks pretty clean because I worked really hard on it[^1]. But it was quite difficult to fit. One problem with the long-tailedness of the naive approach is that it introduces a sort of multi-modal behaviour. If you have consistent marketing over time and you move beta up, the average sales over time goes up. But average sales over time is what the intercept is supposed to represent. So the intercept and beta can generate some nasty interactions. You don't have much control over them because your prior ensures you will explore some unusual beta values. When the sampler jumps into the beta heavens, the curvature of the parameter space can shift abruptly for all parameters, HMC's step size will not be tuned for this rough patch, and it will register as a divergence.\n",
        "\n",
        "[^1]: Farmed random seeds to get synthetic data that was easy + cranked the `target_accept` way up, both practices I'm not proud of.\n",
        "\n",
        "Let's redo that model and you can see how smooth this prior can be. The multi-modality issue doesn't go away entirely (It's just a part of the parameter space in MMMs after all :() but you start out on better footing. Using the default target accept leaves you with 2 divergences and you don't have to push as high before they disappear entirely, like I show here.\n"
      ],
      "id": "6ba7e96c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_df = pd.read_csv(\"mmm_multidimensional_example.csv\", parse_dates=[\"date\"])\n",
        "data_df.head()"
      ],
      "id": "6584bb8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "beta_prior = LogNormalExp(\n",
        "    mu=Prior(\"Gamma\", mu=0.25, sigma=0.10, dims=(\"channel\")),\n",
        "    sigma=Prior(\"Exponential\", scale=0.10, dims=(\"channel\")),\n",
        "    dims=(\"channel\", \"geo\"),\n",
        "    centered=False,\n",
        ")"
      ],
      "id": "a0304ccd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pymc_marketing.mmm import GeometricAdstock, LogisticSaturation\n",
        "from pymc_marketing.mmm.multidimensional import MMM\n",
        "\n",
        "saturation = LogisticSaturation(\n",
        "    priors={\n",
        "        \"beta\": beta_prior,\n",
        "        \"lam\": Prior(\n",
        "            \"Gamma\",\n",
        "            mu=0.5,\n",
        "            sigma=0.25,\n",
        "            dims=(\"channel\"),\n",
        "        ),\n",
        "    }\n",
        ")\n",
        "\n",
        "adstock = GeometricAdstock(\n",
        "    priors={\"alpha\": Prior(\"Beta\", alpha=2, beta=5, dims=(\"geo\", \"channel\"))}, l_max=8\n",
        ")\n",
        "\n",
        "model_config = {\n",
        "    \"intercept\": Prior(\"Gamma\", mu=0.5, sigma=0.25, dims=\"geo\"),\n",
        "    \"gamma_control\": Prior(\"Normal\", mu=0, sigma=0.5, dims=\"control\"),\n",
        "    \"gamma_fourier\": Prior(\n",
        "        \"Normal\",\n",
        "        mu=0,\n",
        "        sigma=Prior(\"HalfNormal\", sigma=0.2),\n",
        "        dims=(\"geo\", \"fourier_mode\"),\n",
        "        centered=False,\n",
        "    ),\n",
        "    \"likelihood\": Prior(\n",
        "        \"TruncatedNormal\",\n",
        "        lower=0,\n",
        "        sigma=Prior(\"HalfNormal\", sigma=1.5),\n",
        "        dims=(\"date\", \"geo\"),\n",
        "    ),\n",
        "}\n",
        "\n",
        "mmm = MMM(\n",
        "    date_column=\"date\",\n",
        "    target_column=\"y\",\n",
        "    channel_columns=[\"x1\", \"x2\"],\n",
        "    control_columns=[\"event_1\", \"event_2\"],\n",
        "    dims=(\"geo\",),\n",
        "    scaling={\n",
        "        \"channel\": {\"method\": \"max\", \"dims\": ()},\n",
        "        \"target\": {\"method\": \"max\", \"dims\": ()},\n",
        "    },\n",
        "    adstock=adstock,\n",
        "    saturation=saturation,\n",
        "    yearly_seasonality=2,\n",
        "    model_config=model_config,\n",
        ")"
      ],
      "id": "a69fac13",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x_train = data_df.drop(columns=[\"y\"])\n",
        "y_train = data_df[\"y\"]\n",
        "\n",
        "try:\n",
        "    mmm.fit(\n",
        "        X=x_train,\n",
        "        y=y_train,\n",
        "        chains=4,\n",
        "        target_accept=0.95,\n",
        "    )\n",
        "except:\n",
        "    print(\"You still need to finish the class\")"
      ],
      "id": "eefbd115",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(\n",
        "    mmm.idata,\n",
        "    var_names=[\n",
        "        \"adstock_alpha\",\n",
        "        \"gamma_control\",\n",
        "        \"gamma_fourier\",\n",
        "        \"intercept_contribution\",\n",
        "        \"saturation_beta\",\n",
        "        \"saturation_beta_mu\",\n",
        "        \"saturation_beta_sigma\",\n",
        "        \"saturation_lam\",\n",
        "        \"y_sigma\",\n",
        "    ],\n",
        ")"
      ],
      "id": "8d4ad417",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "_ = az.plot_trace(\n",
        "    data=mmm.idata,\n",
        "    var_names=[\n",
        "        \"adstock_alpha\",\n",
        "        \"gamma_control\",\n",
        "        \"gamma_fourier\",\n",
        "        \"intercept_contribution\",\n",
        "        \"saturation_beta\",\n",
        "        \"saturation_beta_mu\",\n",
        "        \"saturation_beta_sigma\",\n",
        "        \"saturation_lam\",\n",
        "        \"y_sigma\",\n",
        "    ],\n",
        "    compact=True,\n",
        "    backend_kwargs={\"figsize\": (15, 15), \"layout\": \"constrained\"},\n",
        ")\n",
        "plt.gcf().suptitle(\"Model Trace\", fontsize=16, fontweight=\"bold\", y=1.03);"
      ],
      "id": "b736626b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}