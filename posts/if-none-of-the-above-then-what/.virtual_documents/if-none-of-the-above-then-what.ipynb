





import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import pymc as pm
import pytensor.tensor as pt
import arviz as az
from scipy import stats
from matplotlib import cm


#| label: fig-skill-distribution
#| fig-cap: Illustration of learning mechanism in the treadmill model

x = np.linspace(0,20,300)
y = stats.gumbel_r(10,1).pdf(x)
y = y / sum(y)

plt.figure(figsize=(6,4))
plt.plot(x,y,color="tab:blue")
plt.axvline(10,ymin=0,ymax=.95,label="z - a",color="tab:orange")
plt.axvline(15,ymin=0,ymax=.06,label="z",color="tab:red")
plt.plot([8.88,11.7],[.01,.01],'--',label="b",color="tab:blue")
plt.xlabel("Skillfulness")
plt.ylabel("Probability Density of Skillfulness in Next Period")
plt.legend();





#| label: fig-treadmill-model
#| fig-cap: Cumulative social learning is sensitive to population size


N = [10000,1000,100,10]

iterations = 30
a = 5.25
b = 1

plt.figure(figsize=(6,4))

for n in N:

    z_bar = 1
    z_bar_record = [z_bar]

    for j in range(iterations):
        z_bar = -a + b*(np.euler_gamma + np.log(n)) + z_bar
        z_bar_record.append(z_bar)

    plt.plot(z_bar_record,label=n)
    
plt.xlabel('Time',fontsize=15)
plt.ylabel('Average Skill',fontsize=15)
plt.plot([0]*iterations,'--',color='grey')
plt.legend();





try:
    dk = pd.read_csv("Kline",sep=";")
except FileNotFoundError:
    dk = pd.read_csv("https://raw.githubusercontent.com/pymc-devs/pymc-resources/main/Rethinking_2/Data/Kline",sep=";")

dk["contact_id"] = [0,0,0,1,1,1,1,0,1,0]
dk['log_pop'] = np.log(dk['population'])
dk['standardized_log_pop'] = (dk['log_pop'] - dk['log_pop'].mean()) / dk['log_pop'].std() 
dk['log_tools'] = np.log(dk['total_tools'])
times = [[3000],[3000],[3000],[3500],[3000],[3350],[600],[3350],[2845],[800]]
times = [[i[0] / 100] for i in times]
dk['initial_settlement'] = times


# set up the Ordinary Differential Equation (ODE) for each island.

from pymc.ode import DifferentialEquation

def henrich_derivative(y,t,p):
    
    a,b,n = p[0],p[1],p[2]
    
    return -a + b*(np.euler_gamma + np.log(n))  + 0*y[0]

times = dk.initial_settlement.values
pop = dk.population.values
tools = dk.total_tools.values

ode_malekula = DifferentialEquation(func=henrich_derivative, times=times[0], n_states=1, n_theta=3)
ode_tikopia = DifferentialEquation(func=henrich_derivative, times=times[1], n_states=1, n_theta=3)
ode_santa_cruz = DifferentialEquation(func=henrich_derivative, times=times[2], n_states=1, n_theta=3)
ode_yap = DifferentialEquation(func=henrich_derivative, times=times[3], n_states=1, n_theta=3)
ode_lau_fiji = DifferentialEquation(func=henrich_derivative, times=times[4], n_states=1, n_theta=3)
ode_trobriand = DifferentialEquation(func=henrich_derivative, times=times[5], n_states=1, n_theta=3)
ode_chuuk = DifferentialEquation(func=henrich_derivative, times=times[6], n_states=1, n_theta=3)
ode_manus = DifferentialEquation(func=henrich_derivative, times=times[7], n_states=1, n_theta=3)
ode_tonga = DifferentialEquation(func=henrich_derivative, times=times[8], n_states=1, n_theta=3)
ode_hawaii = DifferentialEquation(func=henrich_derivative, times=times[9], n_states=1, n_theta=3)


# configure model

with pm.Model() as m0:
    # Specify prior distributions for some of our model parameters
    a = pm.Gamma('a',mu=15,sigma=5)
    b = pm.Gamma('b',mu=1,sigma=2)

    # If we know one of the parameter values, we can simply pass the value.
    s1 = ode_malekula(y0=[0], theta=[a,b,pop[0]])
    s2 = ode_tikopia(y0=[0], theta=[a,b,pop[1]])
    s3 = ode_santa_cruz(y0=[0], theta=[a,b,pop[2]])
    s4 = ode_yap(y0=[0], theta=[a,b,pop[3]])
    s5 = ode_lau_fiji(y0=[0], theta=[a,b,pop[4]])
    s6 = ode_trobriand(y0=[0], theta=[a,b,pop[5]])
    s7 = ode_chuuk(y0=[0], theta=[a,b,pop[6]])
    s8 = ode_manus(y0=[0], theta=[a,b,pop[7]])
    s9 = ode_tonga(y0=[0], theta=[a,b,pop[8]])
    s10 = ode_hawaii(y0=[0], theta=[a,b,pop[9]])
    
    mu = pt.as_tensor([s1,s2,s3,s4,s5,s6,s7,s8,s9,s10])

    Y = pm.Poisson("Y", mu=mu[:,0,0],observed=tools)


# prior predictive check
# you should see most the mass clustered at zero, a bit asymmetrically skewed positive and
# a few outliers with giant amounts of tools

pp = pm.draw(mu,draws=100)
for val in pp[:,:,-1,-1]:
    plt.plot(val,'o',alpha=0.3)


# fit the model

with m0:
    t = pm.sample(tune=1000,draws=1000,target_accept=0.95,nuts_sampler="nutpie")


# resample predictions from the fitted model

post_pred_m0 = pm.sample_posterior_predictive(model=m0,trace=t)


#| label: fig-bespoke-predictions-1
#| fig-cap: Model-based predictions by population size

predictions = post_pred_m0.posterior_predictive['Y'].values.reshape((12000,10))
label_id = dk.culture.values

fig, ax = plt.subplots(figsize=(8,4))

for index in range(len(label_id)):
    ax.text(np.log(dk.population.values)[index], dk.total_tools.values[index], label_id[index], size=12)

for k in range(50):
    i = np.random.randint(low=0,high=4000)
    plt.plot(np.log(dk.population.values),predictions[i],'o',alpha=0.2,color="tab:blue",markersize=5)
    
plt.plot(np.log(dk.population.values),dk.total_tools.values,'o',color="black")
plt.xlabel("Log population size")
plt.ylabel("Number of tools");



