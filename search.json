[
  {
    "objectID": "posts/make-smart-choices/index.html",
    "href": "posts/make-smart-choices/index.html",
    "title": "Make Smart Choices. Use Multilevel Models.",
    "section": "",
    "text": "Two of the more exciting areas of growth in philosophy of science are large-scale automatic text analysis studies and cultural evolutionary models of scientific communities.\nText analysis studies provide really useful ways of summarizing trends both in philosophy of science and the scientific literature. When we do intellectual history with classical methods, we tend to focus on the heavy hitting, widely cited papers. This makes good sense because people can only read so many papers in their short lives. So focus on the big ones. But most papers are not widely cited. They are just normal little contributions. These mundane papers get less attention in intellectual history. They tend to be boring. Understanding what ordinary work looks like is just as important for understanding a period as understanding the revolutionary papers. Machines don’t get bored of reading mundane papers. Hence the appeal of automated text analysis.\nCultural evolutionary models imagine science is shaped by a process akin to natural selection. Some methods spread through the population of scientists. Others die out. If we could figure out the mechanisms that reward certain kinds of work, then selection-based models could provide some understanding of what shapes scientific practice in the long run.\nA natural thought is that these two methods could be brought together. Text analysis provides the data to test, fit, or otherwise evaluate the cultural evolution models. So far, no one has been able to pull this off in a compelling manner. Rafael Ventura has an exciting new pre-print that is a good step in this direction. Ventura is looking at how formal methods have spread over time inside philosophy of science. The goal is to see whether there is any selection for model-based papers over other papers. In other words, is the incentive structure one that rewards modeling throughout philosophy of science?\nHis data collection has two steps. First, he organizes the published philosophy of science literature into 16 topics. The topics are based on co-citation networks. The intuitive picture (I’m no expert in bibliometric techniques so this is loose!) was that two papers that tend to cite the same references will tend to be in the same topic. If two papers cite a bunch of non-overlapping references, they will tend to be in different topics. Second, he classified papers by whether they used formal methods or not and tracked how the frequency of formal methods changes over time in the 16 topics. Full details on the data collection process are found here: http://philsci-archive.pitt.edu/20885/ .\nThe bit that interests me is that choice to put papers into discrete topics based on co-citation. The 16 topics are not, of course, really entirely isolated research programs. Papers across the topics will cite some of the same works, experience some of the same incentive structures, and cultural shifts in one area of philosophy of science tend to diffuse into other areas. So the choice of 16 topics is somewhat artificial. This isn’t to say it’s a bad choice - there will definitely be a lot of cultural selection concentrated inside each topic. But the choice does introduce some limitations on the results. It would be nice if we could find a way around that, a way of quantifying the causal influence between research communities as well as within them. That’s what this post is all about.\nThe past few months, I’ve been playing around with Bayesian statistical packages in Python. It is mostly just for curiosity’s sake. But now when I read papers with data analysis and their underlying data is public, it can be a lot of fun to reanalyze the results. That’s what I’ve done here. Specifically, I’m using a technique from Bayesian statistics called multi-level modeling.\nThe headline is that at least one of the major findings from Ventura’s paper reverses when the same data is reanalyzed with multi-level modeling. He suggests that there is no selection for formal models across all of philosophy of science. But there might be selection for modeling within particular subdisciplines. I find a stronger pattern of selection for formal models at both levels - the whole of philosophy of science and many of the local clusters."
  },
  {
    "objectID": "posts/make-smart-choices/index.html#prior-simulation",
    "href": "posts/make-smart-choices/index.html#prior-simulation",
    "title": "Make Smart Choices. Use Multilevel Models.",
    "section": "Prior simulation",
    "text": "Prior simulation\nWhen working with Bayesian methods, we have to select priors. I used a strategy known as weakly regularizing priors. The goal is to pick out priors that penalize really extreme effects but still allow for enough uncertainty that the data can drive the ultimate analysis. To calibrate the priors before including the data, I sample from my prior distributions and plots 50 lines at a time to give me a sense of what plausible relationships might look like.\nBelow is the priors I ended up using. For the slopes, I assumed a normal distribution centered around 0 with a standard deviation of 0.3. For the intercepts, I assumed a normal distribution centered around 0 with a standard deviation of 3.\n\n# prior simulation\n\nx_prior = np.arange(21)\n\nfor i in range(50):\n    \n    # sample a value from the priors\n\n    a = stats.norm(0,3).rvs()\n    b = stats.norm(0,0.3).rvs()\n    \n    # plug into the inverse logit function\n\n    p = np.exp(a + b*x_prior) / (1 + np.exp(a + b*x_prior))\n\n    plt.plot(p)\n\n\n\n\nLater, we’ll find that my results depart from those found by Ventura in a couple of places. I suspect that is partly due to the influence of the priors. So I want to spend a bit longer justifying mine.\nMany people are skeptical of the use of priors in statistics. Isn’t it cheating to build assumptions into the model, rather than letting the data do the work? The trouble with prior skepticism is that all analyzes use priors, it’s just that Bayesian analysis uses them explicitly. Other modeling techniques will often tacitly assume flat priors on the possible intercept and slopes. Let me show you what the models look like with much flatter priors.\n\n# prior simulation\n\nx_prior = np.arange(21)\n\nfor i in range(50):\n\n    a = stats.norm(0,5).rvs()\n    b = stats.norm(0,2).rvs()\n\n    p = np.exp(a + b*x_prior) / (1 + np.exp(a + b*x_prior))\n\n    plt.plot(p)\n\n\n\n\nHere I expanded the standard of deviation around the intercept and the slope. The irony is that increasing uncertainty at the level of the priors can decrease uncertainty at the level of predictions. Most predicted models here assume really sharp slopes and implausibly faster growth rates for formal methods in philosophy. Most of these lines shoot from the top to the bottom in the span of 5 or 6 years, suggesting that philosophy could have made a complete revolution in methodology. These predictions are implausible. (A careful explanation of why non-informative priors can be problematic can be found in Richard McElreath’s statistical rethinking book https://xcelab.net/rm/statistical-rethinking/). Hence my preference for the weakly informative priors described above."
  },
  {
    "objectID": "posts/make-smart-choices/index.html#sampling",
    "href": "posts/make-smart-choices/index.html#sampling",
    "title": "Make Smart Choices. Use Multilevel Models.",
    "section": "Sampling",
    "text": "Sampling\nThis code fits 16 logistic regressions, one for each topic.\n\nwith pm.Model() as no_pool:\n    \n    # priors\n    \n    a = pm.Normal('a',0,3,shape=16)\n    b = pm.Normal('b',0,0.3,shape=16)\n    \n    # link function\n\n    p = pm.invlogit(a + b*x)\n    \n    # outcome distribution\n\n    y = pm.Binomial('y',p=p,n=n,observed=k)\n    \n    # sampler\n    \n    trace_no_pool = pm.sample(progressbar=False);\n\n\ntrace_no_pool = az.from_json(\"trace_no_pool\")\naz.plot_trace(trace_no_pool);\n\nC:\\Users\\dsaun\\anaconda3\\envs\\pymc_env\\Lib\\site-packages\\arviz\\utils.py:184: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  numba_fn = numba.jit(**self.kwargs)(self.function)\n\n\n\n\n\nThese plots provide a check on whether the computational estimation strategy was effective. There is little to discuss here, except the good sign that nothing blew up during sampling.\n\naz.plot_forest(trace_no_pool,var_names='b',combined=True,hdi_prob=0.95,quartiles=False);\n\n\n\n\nI’m just listing the slope parameters on each regression (ignoring the intercepts for now). Dots represent the highest posterior point estimate for each slope or, simply, the slope of best fit. The lines represent uncertainty - 95% of the posterior distribution fits within the line. This is like a 95% confidence interval in classic statistics.\nThe general pattern is that most topics experience a modest growth trend for formal methods. A few don’t: Confirmation (2), Metaphysics (8), Relativity (10), and Realism (12). (The numbers that index topics in Ventura’s paper and my analysis will be slightly different. I count up from 0 and go through 15. He counts from 1 and goes to 16. I do this just because the Bayesian fitting software uses this convention. So if you are switching between papers, just keep this in mind.)"
  },
  {
    "objectID": "posts/make-smart-choices/index.html#posterior-predictive-checking",
    "href": "posts/make-smart-choices/index.html#posterior-predictive-checking",
    "title": "Make Smart Choices. Use Multilevel Models.",
    "section": "Posterior predictive checking",
    "text": "Posterior predictive checking\nThese numbers are hard to interpret just as numbers. Let’s extract predictions from what the estimated models and compare them against the observed data.\n\npost_pred_no_pool = pm.sample_posterior_predictive(trace_no_pool,model=no_pool,progressbar=False)\n\n\npost_pred_no_pool = az.from_json(\"post_pred_no_pool\")\nf,ax = plt.subplots(4,4,sharex=True,sharey=True,figsize=(12,10))\n\nfor k in range(50):\n    predictions = np.array(post_pred_no_pool.posterior_predictive.y[0][k].T)\n    ceiling = n.T\n    \n    for i in range(4):\n        for j in range(4):\n            \n            proportions = predictions[i*4+j] / ceiling[i*4+j]\n            ax[i][j].plot(proportions,'o',alpha=0.2,color='tab:orange')\n            \nfor i in range(4):\n    for j in range(4):\n        ax[i][j].plot(proportion_per_year.iloc[i*4+j].values,'-',markersize=2)\n\n\n\n\nThis plot displays the predicted proportion of formal papers in each year. Uncertainty is represented by the spread of the dots and their opacity.\nOne good check is whether the model captures a few plausible intuitive stories. For example, topic 11 is on row 3, column 4. It experiences huge growth in formal methods. This is decision theory. So it makes good sense that nearly half of papers published in this area are in the business of building models. (The other half are likely papers reflecting on the methodology or concepts of decision theory.)\nOne thing to notice is that uncertainty at the beginning of the period tends to be pretty large. This makes sense because we have very little data for the beginning of each period. Some of these topics only have a handful of papers published in them in the year 2000. So it’s like the sample size is very small for the beginning. But it tends to grow in the later years so the estimate clamps down.\nThe high beginning uncertainty is also a reason why splitting the community into 16 topics might introduce some artifacts into the statistical analysis. In the year 2000, each subtopic was not as clearly distinguished as they were in, say, 2010. The common narrative is that philosophy of science sorta splintered into many sub-specialties at the end of the very end of the 20th century and then these subdisciplines consolidated during the early 2000s. So we should also expect more overlap in causal selective forces for the early years, something not reflected in this analysis. Instead, we get big initial uncertainty because each subtopic is not well-established yet.\n\na_means = [np.array(trace_no_pool.posterior['a'][0][:,i].mean()) for i in range(16)]\nb_means = [np.array(trace_no_pool.posterior['b'][0][:,i].mean()) for i in range(16)]\na_means = np.array(a_means)\nb_means = np.array(b_means)\n\nf,ax = plt.subplots(4,4,sharex=True,sharey=True,figsize=(12,10))\n\nx_pred = np.arange(21)\n\nfor i in range(4):\n    for j in range(4):\n        a = a_means[i*4+j] \n        b = b_means[i*4+j]\n        p = np.exp(a + b*x_pred) / (1 + np.exp(a + b*x_pred))\n        ax[i][j].plot(p)\n        ax[i][j].plot(proportion_per_year.iloc[i*4+j].values,'o',markersize=3)\n\n\n\n\nA second way of visualizing the model is to strip away the uncertainty and plot the best performing logistic curves for each community. These are what the model thinks the likely trend is in each place."
  },
  {
    "objectID": "posts/make-smart-choices/index.html#prior-simulation-1",
    "href": "posts/make-smart-choices/index.html#prior-simulation-1",
    "title": "Make Smart Choices. Use Multilevel Models.",
    "section": "Prior simulation",
    "text": "Prior simulation\nAgain I conduct a prior simulation. This time is tricker though because I have priors representing the overall population effects and the those priors shape the subsequent topic effects. So we want to get a prior simulation that allows for a wide range of possible population-topic relationships plus the old goal of giving each topic a range of plausible effects.\n\n# prior simulation\n\nf,ax = plt.subplots(4,4,sharex=True,sharey=True,figsize=(10,10))\n\nhamu = stats.norm(0,2).rvs(16)\nhasig = stats.expon(scale=2).rvs(16)\nhbmu = stats.norm(0,0.2).rvs(16)\nhbsig = stats.expon(scale=0.7).rvs(16)\n\nx_prior = np.arange(21)\n\nfor i in range(4): \n    for j in range(4):\n        \n        amu = hamu[i*4+j]\n        asig = hasig[i*4+j]\n        bmu = hbmu[i*4+j]\n        bsig = hbsig[i*4+j]\n        \n        for l in range(30):\n\n            a = stats.norm(amu,asig).rvs()\n            b = stats.norm(bmu,bsig).rvs()\n\n            p = np.exp(a + b*x_prior) / (1 + np.exp(a + b*x_prior))\n\n            ax[i][j].plot(p)"
  },
  {
    "objectID": "posts/make-smart-choices/index.html#sampling-1",
    "href": "posts/make-smart-choices/index.html#sampling-1",
    "title": "Make Smart Choices. Use Multilevel Models.",
    "section": "Sampling",
    "text": "Sampling\n\n#n.T\n\nwith pm.Model() as partial_pool:\n    \n    # hyperparameters\n    \n    hamu = pm.Normal('hamu',0,2)\n    hasig = pm.Exponential('hasig',2)\n    hbmu = pm.Normal('hbmu',0,0.2)\n    hbsig = pm.Exponential('hbsig',0.7)\n    \n    # regular parameters\n\n    a = pm.Normal('a',hamu,hasig,shape=16)\n    b = pm.Normal('b',hbmu,hbsig,shape=16)\n    \n    # link function\n\n    p = pm.invlogit(a + b*x)\n    \n    # outcome distribution\n\n    y = pm.Binomial('y',p=p,n=n,observed=k)\n    \n    # sampler\n    \n    trace_partial_pool = pm.sample(progressbar=False);\n\nLet’s compare the slopes estimated with multi-level models with those estimated by 16 independent models.\n\ntrace_partial_pool = az.from_json(\"trace_partial_pool\")\naz.plot_forest([trace_partial_pool,trace_no_pool],var_names='b',combined=True,hdi_prob=0.95,quartiles=False);\n\n\n\n\nOne thing to notice is that the level of uncertainty has shrunk across the board. In basically every place, the blue line (multi-level / partial pooling) is shorter than the orange line (no pooling). This is because multi-level models can use more information to inform the estimate in each topic. Information is pooled across topics to form better expectations about each individual topic. It’s like we’ve increased our sample size but we never had to collect new data. We just had to use it more efficiently.\nA second thing to notice is that that several topics either switched from a negative to positive slope or they moved closer to positivity. This suggests that the multi-level model is detecting even stronger selection for formal methods than the previous analysis is. Why would this be? I suspect it largely depends on the interaction effect between intercepts and slopes. When the intercept starts really low, we are more likely to estimate a positive slope. When the intercept starts higher up, we are more likely to estimate a neutral or negative slope. The multi-level model now starts most intercepts fairly low. Here’s a comparison of the two estimates by intercept. The blue lines tend to be more starkly negative for topics 2, 8 and 12, allowing their slopes to head toward positivity.\nI plot the intercept comparisons below.\n\naz.plot_forest([trace_partial_pool,trace_no_pool],var_names='a',combined=True,hdi_prob=0.95,quartiles=False);"
  },
  {
    "objectID": "posts/make-smart-choices/index.html#posterior-predictive-checking-1",
    "href": "posts/make-smart-choices/index.html#posterior-predictive-checking-1",
    "title": "Make Smart Choices. Use Multilevel Models.",
    "section": "Posterior predictive checking",
    "text": "Posterior predictive checking\nLet’s look at the prediction plots now and see what changed once we introduced partial pooling.\n\npost_pred_partial_pool = pm.sample_posterior_predictive(trace_partial_pool,model=partial_pool,progressbar=False)\n\n\npost_pred_partial_pool = az.from_json(\"post_pred_partial_pool\")\nf,ax = plt.subplots(4,4,sharex=True,sharey=True,figsize=(10,10))\n\nfor k in range(50):\n    predictions = np.array(post_pred_partial_pool.posterior_predictive.y[0][k].T)\n    ceiling = np.transpose(n)\n    \n    for i in range(4):\n        for j in range(4):\n            \n            proportions = predictions[i*4+j] / ceiling[i*4+j]\n            ax[i][j].plot(proportions,'o',alpha=0.2,color='tab:orange')\n            \nfor i in range(4):\n    for j in range(4):\n        ax[i][j].plot(proportion_per_year.iloc[i*4+j].values,'-',markersize=2)\n\n\n\n\nOne thing to notice that there is less initial uncertainty in many of these plots. This highlights on the advantages of multi-level models - less uncertainty in the intercepts means more plausible estimates of the slope.\n\na_means_pp = [np.array(trace_partial_pool.posterior['a'][0][:,i].mean()) for i in range(16)]\nb_means_pp = [np.array(trace_partial_pool.posterior['b'][0][:,i].mean()) for i in range(16)]\na_means_pp = np.array(a_means_pp)\nb_means_pp = np.array(b_means_pp)\n\nf,ax = plt.subplots(4,4,sharex=True,sharey=True,figsize=(10,10))\n\nx_pred = np.arange(21)\n\nfor i in range(4):\n    for j in range(4):\n        a = a_means_pp[i*4+j] \n        b = b_means_pp[i*4+j]\n        p = np.exp(a + b*x_pred) / (1 + np.exp(a + b*x_pred))\n        ax[i][j].plot(p)\n        ax[i][j].plot(proportion_per_year.iloc[i*4+j].values,'o',markersize=3)\n\n\n\n\nThere are small divergences in the trend lines. I’ll zoom in one difference in the last section.\nFinally, we have the trend line for the entire population of studies. It is subtly but confidently positive, suggesting an overall tendency to select for formal methods in philosophy of science over the last several years. The observed trends are plotted and faded behind it.\n\na = np.array(trace_partial_pool.posterior['hamu'].mean((\"chain\",\"draw\")))\nb = np.array(trace_partial_pool.posterior['hbmu'].mean((\"chain\",\"draw\")))\n\np = np.exp(a + b*x_pred) / (1 + np.exp(a + b*x_pred))\n\nfor i in range(4):\n    for j in range(4):\n        plt.plot(proportion_per_year.iloc[i*4+j].values,'-',alpha=0.4)\n\nplt.plot(p,color=\"black\")\nplt.ylim([0,1]);\n\n\n\n\nVentura’s analysis noted that there was no overall selection for formal methods across philosophy of science. Interestingly, my analysis finds a small and very confident positive slope for the population-level effect. The population-level slope is 0.018 with the bottom of the credibility interval at 0.002 and the top at 0.034. So under classical statistics, this would be like a statistically significant effect.\nShould we prefer the multi-level trend estimate? I suspect so but the reason lies in how multi-level models handle each individual cluster. So I’ll turn to that next."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "imagination machine",
    "section": "",
    "text": "If none of the above, then what?\n\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2023\n\n\nDaniel Saunders\n\n\n\n\n\n\n  \n\n\n\n\nMake Smart Choices. Use Multilevel Models.\n\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\nDaniel Saunders\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I write about philosophy of science, cultural evolution and Bayesian statistics."
  },
  {
    "objectID": "posts/if-none-of-the-above-then-what/index.html",
    "href": "posts/if-none-of-the-above-then-what/index.html",
    "title": "If none of the above, then what?",
    "section": "",
    "text": "A couple days ago, Richard McElreath had a lovely blog post on why the Bayes factor isn’t the cure to what ails us. No p-values or No confidence intervals, either. No metric of any kind can replace the need for clear model-first thinking as the justification for scientific claims. A common reaction to Richard’s post is to ask the obvious question, “if none of the above, then what?” The question underscores just how difficult it is to imagine another kind of science.\nA casual aquaintance with the p-value wars of the last decade leaves one with the impression that we are searching for some metric which we can stamp on a scientific paper to let everyone know we did a good job. If not the p-value, then surely something else: a cross-validation score, a low AIC score, a big effect size, a big likelihood ratio. Maybe a good paper is the one that has (N = 1,000,000,000) in the abstract! There is a pretense to these conversations: if we just tweak our metric correctly, we can also tweak the publication pipeline and stabilize science. Bayesians in Richard’s camp are asking a provocative question - what if the pretense is wrong? How could we justify a scientific claim in the absence of any of the metrics?\nIt turns out the answer is not mysticism. A humble cycle of model-building and fitting is usually all we need. The metrics listed above have their proper place. It’s just that they are rarely what is called for. A model-first approach makes it obvious when they are useful and how to deploy them. The philosophical arguments for this approach have been spelled out before (Gelman and Shalizi 2013). But it is still quite hard to imagine what it looks like from these abstract methodological descriptions. The rest of this post provides a compact and easy to follow example of the full model-first workflow. The promise is that, by the end, we won’t have needed a metric to assess our model and you won’t miss them. We’ll learn something important without ever reaching for them.\nWe are going to study a model of technological innovation. The central claim is that big populations innovate more than small populations. We’ll build up the model from basic intuitions and evaluate it against a historical dataset of island societies in Oceania. It will feel a bit too easy and that’s the point. Once we carefully model our problem, there is no need to argue about p-values, bayes factors, confidence intervals or the like."
  },
  {
    "objectID": "posts/if-none-of-the-above-then-what/index.html#footnotes",
    "href": "posts/if-none-of-the-above-then-what/index.html#footnotes",
    "title": "If none of the above, then what?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor many technologies, there will not be some continuous improvement that can be equated directly with a skill level. Instead, there is some further function that maps skillfulness onto discrete stages of improvement in a gadget or even the variety of gadgets a person can produce. Henrich sets this complexity aside to illustrate a general relationship that should hold approximately in the complex case and we’ll do the same I think this is a pretty big simplification. Simplifications like this serve as an opportunity for future work, fleshing out the theory and seeing what is plausible in light of empirical evidence. To be fair, people have challenged Henrich on this one (Vaesen et al. 2016).↩︎\nIf you are wondering whether we can reliably measure the complexity of a tool based on ethnographic reports written during the period of European colonization, you are not alone (Jerven 2011). But the part of this case study that is interesting for our purposes does not concern data quality. I will assume these measures are appropriate for the sake of the larger argument.↩︎\nWe don’t have to use a differential equation solver. We could just integrate the differential equation to get an analytical expression for the number of tools as function of \\(a,b,n,t\\). It would be faster and more accurate too. But I had already worked out the differential equation approach by the time I realized it was an easy integration problem so here’s what we get. Still nice to know the fitting differential equations ain’t too hard.↩︎"
  }
]