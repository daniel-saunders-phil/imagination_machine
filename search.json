[
  {
    "objectID": "posts/make-smart-choices/Partial_pooling_ventura_blog.html",
    "href": "posts/make-smart-choices/Partial_pooling_ventura_blog.html",
    "title": "Make Smart Choices, Use Multi-Level Models",
    "section": "",
    "text": "Two of the more exciting areas of growth in philosophy of science are large-scale automatic text analysis studies and cultural evolutionary models of scientific communities.\nText analysis studies provide really useful ways of summarizing trends both in philosophy of science and the scientific literature. When we do intellectual history with classical methods, we tend to focus on the heavy hitting, widely cited papers. This makes good sense because people can only read so many papers in their short lives. So focus on the big ones. But most papers are not widely cited. They are just normal little contributions. These mundane papers get less attention in intellectual history. They tend to be boring. Understanding what ordinary work looks like is just as important for understanding a period as understanding the revolutionary papers. Machines don’t get bored of reading mundane papers. Hence the appeal of automated text analysis.\nCultural evolutionary models imagine science is shaped by a process akin to natural selection. Some methods spread through the population of scientists. Others die out. If we could figure out the mechanisms that reward certain kinds of work, then selection-based models could provide some understanding of what shapes scientific practice in the long run.\nA natural thought is that these two methods could be brought together. Text analysis provides the data to test, fit, or otherwise evaluate the cultural evolution models. So far, no one has been able to pull this off in a compelling manner. Rafael Ventura has an exciting new pre-print that is a good step in this direction. Ventura is looking at how formal methods have spread over time inside philosophy of science. The goal is to see whether there is any selection for model-based papers over other papers. In other words, is the incentive structure one that rewards modeling throughout philosophy of science?\nHis data collection has two steps. First, he organizes the published philosophy of science literature into 16 topics. The topics are based on co-citation networks. The intuitive picture (I’m no expert in bibliometric techniques so this is loose!) was that two papers that tend to cite the same references will tend to be in the same topic. If two papers cite a bunch of non-overlapping references, they will tend to be in different topics. Second, he classified papers by whether they used formal methods or not and tracked how the frequency of formal methods changes over time in the 16 topics. Full details on the data collection process are found here: http://philsci-archive.pitt.edu/20885/.\nThe bit that interests me is that choice to put papers into discrete topics based on co-citation. The 16 topics are not, of course, really entirely isolated research programs. Papers across the topics will cite some of the same works, experience some of the same incentive structures, and cultural shifts in one area of philosophy of science tend to diffuse into other areas. So the choice of 16 topics is somewhat artificial. This isn’t to say it’s a bad choice - there will definitely be a lot of cultural selection concentrated inside each topic. But the choice does introduce some limitations on the results. It would be nice if we could find a way around that, a way of quantifying the causal influence between research communities as well as within them. That’s what this post is all about.\nThe past few months, I’ve been playing around with Bayesian statistical packages in Python. It is mostly just for curiosity’s sake. But now when I read papers with data analysis and their underlying data is public, it can be a lot of fun to reanalyze the results. That’s what I’ve done here. Specifically, I’m using a technique from Bayesian statistics called multi-level modeling.\nThe headline is that at least one of the major findings from Ventura’s paper reverses when the same data is reanalyzed with multi-level modeling. He suggests that there is no selection for formal models across all of philosophy of science. But there might be selection for modeling within particular subdisciplines. I find a stronger pattern of selection for formal models at both levels - the whole of philosophy of science and many of the local clusters."
  },
  {
    "objectID": "posts/make-smart-choices/Partial_pooling_ventura_blog.html#prior-simulation",
    "href": "posts/make-smart-choices/Partial_pooling_ventura_blog.html#prior-simulation",
    "title": "Make Smart Choices, Use Multi-Level Models",
    "section": "Prior simulation",
    "text": "Prior simulation\nWhen working with Bayesian methods, we have to select priors. I used a strategy known as weakly regularizing priors. The goal is to pick out priors that penalize really extreme effects but still allow for enough uncertainty that the data can drive the ultimate analysis. To calibrate the priors before including the data, I sample from my prior distributions and plots 50 lines at a time to give me a sense of what plausible relationships might look like.\nBelow is the priors I ended up using. For the slopes, I assumed a normal distribution centered around 0 with a standard deviation of 0.3. For the intercepts, I assumed a normal distribution centered around 0 with a standard deviation of 3.\n\n\n\n\n\nLater, we’ll find that my results depart from those found by Ventura in a couple of places. I suspect that is partly due to the influence of the priors. So I want to spend a bit longer justifying mine.\nMany people are skeptical of the use of priors in statistics. Isn’t it cheating to build assumptions into the model, rather than letting the data do the work? The trouble with prior skepticism is that all analyzes use priors, it’s just that Bayesian analysis uses them explicitly. Other modeling techniques will often tacitly assume flat priors on the possible intercept and slopes. Let me show you what the models look like with much flatter priors.\n\n\n\n\n\nHere I expanded the standard of deviation around the intercept and the slope. The irony is that increasing uncertainty at the level of the priors can decrease uncertainty at the level of predictions. Most predicted models here assume really sharp slopes and implausibly faster growth rates for formal methods in philosophy. Most of these lines shoot from the top to the bottom in the span of 5 or 6 years, suggesting that philosophy could have made a complete revolution in methodology. These predictions are implausible. (A careful explanation of why non-informative priors can be problematic can be found in Richard McElreath’s statistical rethinking book https://xcelab.net/rm/statistical-rethinking/). Hence my preference for the weakly informative priors described above."
  },
  {
    "objectID": "posts/make-smart-choices/Partial_pooling_ventura_blog.html#sampling",
    "href": "posts/make-smart-choices/Partial_pooling_ventura_blog.html#sampling",
    "title": "Make Smart Choices, Use Multi-Level Models",
    "section": "Sampling",
    "text": "Sampling\nThis code fits 16 logistic regressions, one for each topic.\n\n\n\n\n\nThese plots provide a check on whether the computational estimation strategy was effective. There is little to discuss here, except the good sign that nothing blew up during sampling.\n\n\n\n\n\nI’m just listing the slope parameters on each regression (ignoring the intercepts for now). Dots represent the highest posterior point estimate for each slope or, simply, the slope of best fit. The lines represent uncertainty - 95% of the posterior distribution fits within the line. This is like a 95% confidence interval in classic statistics.\nThe general pattern is that most topics experience a modest growth trend for formal methods. A few don’t: Confirmation (2), Metaphysics (8), Relativity (10), and Realism (12). (The numbers that index topics in Ventura’s paper and my analysis will be slightly different. I count up from 0 and go through 15. He counts from 1 and goes to 16. I do this just because the Bayesian fitting software uses this convention. So if you are switching between papers, just keep this in mind.)"
  },
  {
    "objectID": "posts/make-smart-choices/Partial_pooling_ventura_blog.html#posterior-predictive-checking",
    "href": "posts/make-smart-choices/Partial_pooling_ventura_blog.html#posterior-predictive-checking",
    "title": "Make Smart Choices, Use Multi-Level Models",
    "section": "Posterior predictive checking",
    "text": "Posterior predictive checking\nThese numbers are hard to interpret just as numbers. Let’s extract predictions from what the estimated models and compare them against the observed data.\n\n\nSampling: [y]\n\n\n\n\n\n\n\nThis plot displays the predicted proportion of formal papers in each year. Uncertainty is represented by the spread of the dots and their opacity.\nOne good check is whether the model captures a few plausible intuitive stories. For example, topic 11 is on row 3, column 4. It experiences huge growth in formal methods. This is decision theory. So it makes good sense that nearly half of papers published in this area are in the business of building models. (The other half are likely papers reflecting on the methodology or concepts of decision theory.)\nOne thing to notice is that uncertainty at the beginning of the period tends to be pretty large. This makes sense because we have very little data for the beginning of each period. Some of these topics only have a handful of papers published in them in the year 2000. So it’s like the sample size is very small for the beginning. But it tends to grow in the later years so the estimate clamps down.\nThe high beginning uncertainty is also a reason why splitting the community into 16 topics might introduce some artifacts into the statistical analysis. In the year 2000, each subtopic was not as clearly distinguished as they were in, say, 2010. The common narrative is that philosophy of science sorta splintered into many sub-specialties at the end of the very end of the 20th century and then these subdisciplines consolidated during the early 2000s. So we should also expect more overlap in causal selective forces for the early years, something not reflected in this analysis. Instead, we get big initial uncertainty because each subtopic is not well-established yet.\n\n\n\n\n\nA second way of visualizing the model is to strip away the uncertainty and plot the best performing logistic curves for each community. These are what the model thinks the likely trend is in each place."
  },
  {
    "objectID": "posts/make-smart-choices/Partial_pooling_ventura_blog.html#prior-simulation-1",
    "href": "posts/make-smart-choices/Partial_pooling_ventura_blog.html#prior-simulation-1",
    "title": "Make Smart Choices, Use Multi-Level Models",
    "section": "Prior simulation",
    "text": "Prior simulation\nAgain I conduct a prior simulation. This time is tricker though because I have priors representing the overall population effects and the those priors shape the subsequent topic effects. So we want to get a prior simulation that allows for a wide range of possible population-topic relationships plus the old goal of giving each topic a range of plausible effects."
  },
  {
    "objectID": "posts/make-smart-choices/Partial_pooling_ventura_blog.html#sampling-1",
    "href": "posts/make-smart-choices/Partial_pooling_ventura_blog.html#sampling-1",
    "title": "Make Smart Choices, Use Multi-Level Models",
    "section": "Sampling",
    "text": "Sampling\nLet’s compare the slopes estimated with multi-level models with those estimated by 16 independent models.\n\n\n\n\n\nOne thing to notice is that the level of uncertainty has shrunk across the board. In basically every place, the blue line (multi-level / partial pooling) is shorter than the orange line (no pooling). This is because multi-level models can use more information to inform the estimate in each topic. Information is pooled across topics to form better expectations about each individual topic. It’s like we’ve increased our sample size but we never had to collect new data. We just had to use it more efficiently.\nA second thing to notice is that that several topics either switched from a negative to positive slope or they moved closer to positivity. This suggests that the multi-level model is detecting even stronger selection for formal methods than the previous analysis is. Why would this be? I suspect it largely depends on the interaction effect between intercepts and slopes. When the intercept starts really low, we are more likely to estimate a positive slope. When the intercept starts higher up, we are more likely to estimate a neutral or negative slope. The multi-level model now starts most intercepts fairly low. Here’s a comparison of the two estimates by intercept. The blue lines tend to be more starkly negative for topics 2, 8 and 12, allowing their slopes to head toward positivity.\nI plot the intercept comparisons below."
  },
  {
    "objectID": "posts/make-smart-choices/Partial_pooling_ventura_blog.html#posterior-predictive-checking-1",
    "href": "posts/make-smart-choices/Partial_pooling_ventura_blog.html#posterior-predictive-checking-1",
    "title": "Make Smart Choices, Use Multi-Level Models",
    "section": "Posterior predictive checking",
    "text": "Posterior predictive checking\nLet’s look at the prediction plots now and see what changed once we introduced partial pooling.\n\n\nSampling: [y]\n\n\n\n\n\n\n\nOne thing to notice that there is less initial uncertainty in many of these plots. This highlights on the advantages of multi-level models - less uncertainty in the intercepts means more plausible estimates of the slope.\n\n\n\n\n\nThere are small divergences in the trend lines. I’ll zoom in one difference in the last section.\nFinally, we have the trend line for the entire population of studies. It is subtly but confidently positive, suggesting an overall tendency to select for formal methods in philosophy of science over the last several years. The observed trends are plotted and faded behind it.\n\n\n\n\n\nVentura’s analysis noted that there was no overall selection for formal methods across philosophy of science. Interestingly, my analysis finds a small and very confident positive slope for the population-level effect. The population-level slope is 0.018 with the bottom of the credibility interval at 0.002 and the top at 0.034. So under classical statistics, this would be like a statistically significant effect.\nShould we prefer the multi-level trend estimate? I suspect so but the reason lies in how multi-level models handle each individual cluster. So I’ll turn to that next."
  },
  {
    "objectID": "posts/how-to-sample-an-mmm-as-fast-as-possible/index.html",
    "href": "posts/how-to-sample-an-mmm-as-fast-as-possible/index.html",
    "title": "How to sample an MMM as fast as possible",
    "section": "",
    "text": "You find yourself in a gorgeous Airbnb. You are on a Zoom meeting, hiding the Wayfair decor behind a virtual background, a blurry image of your actual office. You are embarrassed. You forgot to tell your boss you booked a vacation. They asked for preliminary results they can share with their boss tomorrow morning. You just needed 1,000 usable samples from the posterior of this MMM. But you don’t have them. Hamiltonian Monte Carlo glided through the parameter space like a snail on a hot sidewalk. The progress bar told you 10 minutes. Then 10 hours. You ended up waiting an hour and a half for these samples. When your results came back, the effective sample size diagnostic said you only have 80 good samples. Your boss wants answers. Their boss wants answers. You promise to get better results and hang up the call. Your palms are sweaty. Your brow is damp. You crank the number of draws up to 10,000, slam ctrl+enter, and pray the inference doesn’t cook your processor overnight.\nThe promise of probabilistic programming systems is that modelers can focus on the model and ignore the messy details of inference. This puts modelers in an awkward spot when the sampler underperforms. You have been told to almost always use the default settings of the sampler and have developed a mental block against learning about the details of how Hamiltonian Monte Carlo (HMC) actually works. You know the folk theorem of statistical computing: “the problem isn’t the sampler, the problem is your model”, blah blah blah. But you cannot change the model. The Stakeholders asked for an MMM, and an MMM shall be delivered.\nI think it’s quite common that MMMs need non-default settings for inference. The challenges with sampling them do not arise from the usual places where you can make tweaks: choices of prior families, parameterizations, that sort of stuff. This is exactly the edge case where the modeler needs to know the details of how inference works and use non-default settings. Unfortunately, if you follow the folk theorem too stringently, you’ll waste hours trying to “fix” the model.\nOur plan is to figure out where those 920 missing samples went. I’ll show you that the default inference settings struggle with the posterior geometry of MMMs, even in simple models with good data. MMMs naturally have posterior correlations between their parameters and strong correlations are difficult to sample from. That will lead us to a conversation about how to optimize HMC for MMMs and some nice intuitions for the behaviour of different mass matrix adaptation schemes."
  },
  {
    "objectID": "posts/how-to-sample-an-mmm-as-fast-as-possible/index.html#footnotes",
    "href": "posts/how-to-sample-an-mmm-as-fast-as-possible/index.html#footnotes",
    "title": "How to sample an MMM as fast as possible",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA cool and non-obvious reason to use non-uniform priors is precisely because it mitigates the problem I’m exploring in this post. The correlation in the posterior is entirely driven by the correlation in the likelihood surface. But there is no correlation between the priors; they are independent. Including prior information partially de-correlates the posterior.↩︎\nHMC tuning is trying to balance the amount of error in its numerical simulation of trajectories against the goal of efficiency. A tiny step size has almost no error. HMC has to keep the error in an acceptable range across all parameters. So it tends to be the most difficult parameter to sample that controls the tuning process.↩︎\nAt least Stan, Nutpie, PyMC, and Numpyro have this default.↩︎"
  },
  {
    "objectID": "posts/break-your-toys-and-put-them-back-together/break-your-toys-and-glue-them-back-together.html",
    "href": "posts/break-your-toys-and-put-them-back-together/break-your-toys-and-glue-them-back-together.html",
    "title": "Break your toys and glue them back together",
    "section": "",
    "text": "To me, the most interesting question in the social sciences is what statistical practice will look like in 20 years. Criticisms of null hypothesis significance testing has become mainstream. Even if p-values still dominant the publication landscape, many of other approaches to analysis are making it into major journals. At the same time, the software that lets people build more customizable, interesting models has been getting faster and more accessible. Sometimes it feels like our biggest challenge is that we don’t quite know what to do with all this power. As a community we need more examples of what another statistical practice might look like.\nI previously wrote about a case where I took a toy mechanistic model and fit it to the small dataset with 10 islands in Oceania. What’s cool about this sort of analysis is that you don’t need to substitute out your mechanistic model for a linear regression when performing data analysis. That sort of substitution can introduce all sorts of complications in reasoning from one model to another. The approach where you fit a mechanistic model is much clearer and, when the model inevitably fails, the failures take on a importance within the framework of the theory.\nThis post is a sequel. I’m going to continue working with that case study and show a small bit about how to iteratively tailor a toy mechanistic model to the messiness of the world."
  },
  {
    "objectID": "posts/break-your-toys-and-put-them-back-together/break-your-toys-and-glue-them-back-together.html#fitting",
    "href": "posts/break-your-toys-and-put-them-back-together/break-your-toys-and-glue-them-back-together.html#fitting",
    "title": "Break your toys and glue them back together",
    "section": "Fitting",
    "text": "Fitting\nWe’ll import the same data.\n\n\n\n\n\n\n\n\n\nculture\npopulation\ncontact\ntotal_tools\nmean_TU\ninitial_settlement (BP)\n\n\n\n\n0\nMalekula\n1100\nlow\n13\n3.2\n3000\n\n\n1\nTikopia\n1500\nlow\n22\n4.7\n3000\n\n\n2\nSanta Cruz\n3600\nlow\n24\n4.0\n3000\n\n\n3\nYap\n4791\nhigh\n43\n5.0\n3500\n\n\n4\nLau Fiji\n7400\nhigh\n33\n5.0\n3000\n\n\n5\nTrobriand\n8000\nhigh\n19\n4.0\n3350\n\n\n6\nChuuk\n9200\nhigh\n40\n3.8\n600\n\n\n7\nManus\n13000\nlow\n28\n6.6\n3350\n\n\n8\nTonga\n17500\nhigh\n55\n5.4\n2845\n\n\n9\nHawaii\n275000\nlow\n71\n6.6\n800\n\n\n\n\n\n\n\nRename our variables to T for time, N for population size, and tools for the total tools present in a society.\nThe model can be expressed compactly with:\nThe priors need some justification. First, the Gamma distributions have really small means. That is because the population and time values are quite large. If you push their values up a bit, you’d have explosive growth in technology. Second, I tuned the priors by looking at prior predictive simulations. I want the bulk of the prior predictions to be below 1000. Some of our islands only have about 1000 people. It is certainly possible that everyone on an island is an inventor but it is unlikely. I simply took draws from the prior and kept adjusting the parameters until the growth rates were fairly stable and represented wide uncertainty.\n\n\n\n\n\nWe face a bit of challenge in fitting the model - the parameters \\(a\\) and \\(b\\) are collinear. Each represents a certain kind of difficulty and they can counterbalance each other. If \\(a\\) is high, the skill is uniformly difficult to learn. But if \\(b\\) is also high, it makes up for that because it provides more opportunities for novel innovation. So there are several combinations of \\(a\\) and \\(b\\) parameters that provide the same accuracy in predicting the oceania dataset. The figure below illustrates how the MCMC algorithm explores the parameter space. It travels back and forth on this narrow band of roughly equivalent probability.\n\n\nC:\\Users\\dsaun\\anaconda3\\envs\\pymc_env\\Lib\\site-packages\\arviz\\plots\\pairplot.py:232: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n  gridsize = int(dataset.dims[\"draw\"] ** 0.35)\n\n\n\n\n\nCollinearity is a medium-sized problem. It will inflate the uncertainty on our parameters. If we really cared to pin down the value of \\(a\\), we’d need extra information to first pin down the value of \\(b\\), or visa-versa. Or, we should scrape this model and build a new one. In our case, I don’t particularly care what value each parameter takes on. I’m more interested in whether our model can explain the available data.\nBelow I plot the predictions the model makes about island societies, given the estimated parameter values. I’ve plotted 50 random predictions to help represent the uncertainty propagating through the model. The model performs pretty well on 6 of the island societies, suggesting it could be a candidate explanation for dynamics of technical development in those societies. However, in Hawaii, Manus, Trobriand and Chuuk, the model doesn’t have a good way to predict them. Hawaii and Chuuk both have more tools that they are supposed, given their development time and population size. Meanwhile, Manus and Trobriand have too few tools.\nAt this point, it doesn’t look like the demographic theory of technology is very plausible. The worry persists even if you dealt with the collinearity problem. If you pushed the growth rate up a little bit, you’d over shoot Trobriand and Manus even more. If you pushed the growth rate down a little, the problem of Hawaii and Chuuk gets worse.\n\n\nSampling: [Y]\n\n\n\n\n\nWhen it comes to fitting toy mechanistic models we have to be careful. They break more easily than linear regression. Frankly, I think that’s a good thing. If your model fits every dataset, you cannot learn from it. However! If a toy model breaks for opaque reasons, then it is also hard to learn from. In our case, it’s not initially easy to tell whether the demographic theory of technology is bunk or whether there are some simple features of the Oceanic societies that we have to add before the model works well.\nThe world is a complicated place. Many of those complications are case-specific - there are unique features of the history and geography of Oceania that influence the distribution of technology. Toy models are usually developed to capture some core general features that should recur across a range of cases. Out-of-the-box they usually don’t fit any cases well.\nThe way I envision a productive scientific workflow is this: first we fit the toy model. Then we slowly layer on extra features to tailor it to some specific domain. If the toy model actually does capture some core mechanism in the world, it should start to fit nicely after a bit of tailoring. If the fit of the model doesn’t improve too much once we add the extra features, we should think there is something wrong with the core theory."
  },
  {
    "objectID": "posts/break-your-toys-and-put-them-back-together/break-your-toys-and-glue-them-back-together.html#a-surprisingly-compact-expression",
    "href": "posts/break-your-toys-and-put-them-back-together/break-your-toys-and-glue-them-back-together.html#a-surprisingly-compact-expression",
    "title": "Break your toys and glue them back together",
    "section": "A surprisingly compact expression",
    "text": "A surprisingly compact expression\nFirst, we’ll expand the second term. That means multiply the difference equation by \\(T_{f} - T_{s}\\)\n\\[ (-a + b*(\\gamma + \\ln(n_{s})))*T_{s} + (-a + b*(\\gamma + \\ln(n_{f})))*T_{f} - (-a + b*(\\gamma + \\ln(n_{f})))*T_{s} \\]\nNotice that the first and the last terms of the equations are almost the same - they have the core difference equation multiplied \\(T_{s}\\). The only difference is that one uses the population of the settled society, \\(\\ln(n_{s})\\), and the other uses the population of the founding society, \\(\\ln(n_{f})\\). It will turn out we can exploit this fact to a lot of simplification. What is left will show that we only need to care about the ratio of population sizes to incorporate founder effects. It will also mean that we can leave the middle term completely untouched.\nLet’s distribute the time terms on the left and right sides of this expression.\n\\[ (-aT_{s}  + bT_{s}(\\gamma + \\ln(n_{s})))+ (-a + b(\\gamma + \\ln(n_{f})))T_{f} - (-aT_{s} + bT_{s}(\\gamma + \\ln(n_{f}))) \\]\nMultiply the third term by negative 1 to take care of the minus sign in front.\n\\[ -aT_{s}  + bT_{s}(\\gamma + \\ln(n_{s})) + (-a + b(\\gamma + \\ln(n_{f})))T_{f} + aT_{s} - bT_{s}(\\gamma + \\ln(n_{f})) \\]\nThe \\(-aT_{s}\\) term in the front cancels out the \\(aT_{s}\\) term toward the end.\n\\[  bT_{s}(\\gamma + \\ln(n_{s})) + (-a + b(\\gamma + \\ln(n_{f})))T_{f} - bT_{s}(\\gamma + \\ln(n_{f}))\\]\nDistribute the \\(bT_{s}\\) terms at the front and back.\n\\[  bT_{s}\\gamma + bT_{s}\\ln(n_{s}) + (-a + b*(\\gamma + \\ln(n_{f})))T_{f} - bT_{s}\\gamma - bT_{s}\\ln(n_{f})\\]\nThe \\(bT_{s}\\gamma\\)’s cancel.\n\\[  bT_{s}\\ln(n_{s}) + (-a + b*(\\gamma + \\ln(n_{f})))T_{f} - bT_{s}\\ln(n_{f})\\]\nRearrange the expression to move the \\(bT_{s}\\ln(n_{f})\\) term to the front.\n\\[  bT_{s}\\ln(n_{s}) - bT_{s}\\ln(n_{f}) + (-a + b(\\gamma + \\ln(n_{f})))T_{f} \\]\nFactor out the \\(bT_{s}\\) term.\n\\[  bT_{s}(\\ln(n_{s}) - \\ln(n_{f})) + (-a + b(\\gamma + \\ln(n_{f})))T_{f} \\]\nLastly, use the fact that the difference between logarithms is just the logarithm of the quotient.\n\\[  bT_{s}(\\ln(n_{s} / n_{f})) + (-a + b(\\gamma + \\ln(n_{f})))T_{f} \\]\nWe’re done. We’ve arrived at what I call the founder model. The second half of the expression is the familiar expression for the founder society - it tells us what happens if we just let the founder society do all the innovating. The first half of the expression adjusts the original model based on how much time the settled society has been around plus a ratio of their population sizes. When the settled society is bigger, the adjustment factor is positive. When the settled society is smaller, the adjustment factor is negative."
  },
  {
    "objectID": "posts/break-your-toys-and-put-them-back-together/break-your-toys-and-glue-them-back-together.html#fitting-the-founder-model",
    "href": "posts/break-your-toys-and-put-them-back-together/break-your-toys-and-glue-them-back-together.html#fitting-the-founder-model",
    "title": "Break your toys and glue them back together",
    "section": "Fitting the founder model",
    "text": "Fitting the founder model\nAppreciably, this model assumes that each island immediately takes on its mature population size upon being settled. That’s silly. Population size is also a dynamic process. So some fairly important features of the world are left out. If you feel like that’s too big of an omission, I can understand. At the very least it makes salient exactly why this sort of analysis cannot succeed without substantially more data and thinking. I care a lot more about understanding the techniques and capacities of mathematical modeling than actually answering whether the demographic theory is any good.\nTo program it up, it’s a bit tedious. We’ll need a unique equation for each island and lot of indexing. Regardless, we get the luxury of keeping just two free parameters.\nDespite the strange functional specification, the model fits as easily as the first.\nWe can now visualize the predictions. The model does a lot better explaining those islands that previously had too many tools. Hawaii and Chuuk are now squarely with the range of model uncertainties. Manus and Trobriand are still a bit stubborn. This makes good sense - the structural changes we made meant that islands will, in general, have more tools than before. So if Trobriand and Manus previously had too few tools to be explained, we don’t have many new tools to explain them.\n\n\nSampling: [Y]\n\n\n\n\n\nOur visual intuition that the founder model is doing better is validated by model comparison statistics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC:\\Users\\dsaun\\anaconda3\\envs\\pymc_env\\Lib\\site-packages\\arviz\\stats\\stats.py:805: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\nC:\\Users\\dsaun\\anaconda3\\envs\\pymc_env\\Lib\\site-packages\\arviz\\stats\\stats.py:1040: RuntimeWarning: overflow encountered in exp\n  weights = 1 / np.exp(len_scale - len_scale[:, None]).sum(axis=1)\nC:\\Users\\dsaun\\anaconda3\\envs\\pymc_env\\Lib\\site-packages\\arviz\\stats\\stats.py:805: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\nC:\\Users\\dsaun\\anaconda3\\envs\\pymc_env\\Lib\\site-packages\\arviz\\stats\\stats.py:309: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'True' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df_comp.loc[val] = (\nC:\\Users\\dsaun\\anaconda3\\envs\\pymc_env\\Lib\\site-packages\\arviz\\stats\\stats.py:309: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'log' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df_comp.loc[val] = (\n\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nStandard model\n0\n-114.009490\n17.665856\n0.000000\n1.000000e+00\n34.142028\n0.000000\nTrue\nlog\n\n\nFounder model\n1\n-1277.191116\n1237.180892\n1163.181626\n2.771117e-13\n218.882298\n200.648239\nTrue\nlog\n\n\n\n\n\n\n\nWhat I really like about this strategy of modeling is that we’ve improve the fit just by incorporating domain knowledge. Often, in statistical modeling, the techniques to improve fit involve making the model more flexible. With more adjustable parameters, a model will always fit better. But the improvement in fit doesn’t mean we are discovering the true mechanical structure behind the data. It often only means our model is more flexible. But here we didn’t increase the parametric flexibility. All we did was thought about the problem and did algebra. I think that’s an underappreciated but highly powerful technique.\nI hope my position has become clear - toy models are easy to break but we should break them. Only by breaking them do we get our most informative analyses. When we slowly try to put them back together, we can often come up with clever structural adjustments to the model that improve fit without introducing inappropriate sorts of flexibility."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Imagination Machine",
    "section": "",
    "text": "A positive constrained non-centered prior that sparks joy\n\n\n\n\n\nUnsurprisingly, it makes MMMs faster and more stable.\n\n\n\n\n\n\nAug 14, 2025\n\n\nDaniel Saunders\n\n\n\n\n\n\n  \n\n\n\n\nHow to sample an MMM as fast as possible\n\n\n\n\n\nAnd learn about HMC’s mass matrix along the way.\n\n\n\n\n\n\nMay 31, 2025\n\n\nDaniel Saunders\n\n\n\n\n\n\n  \n\n\n\n\nGeometric intuition for media mix models\n\n\n\n\n\nNot just fancy linear regressions.\n\n\n\n\n\n\nApr 21, 2025\n\n\nDaniel Saunders\n\n\n\n\n\n\n  \n\n\n\n\nBreak your toys and glue them back together\n\n\n\n\n\nMore science, even less p-values.\n\n\n\n\n\n\nSep 13, 2023\n\n\nDaniel Saunders\n\n\n\n\n\n\n  \n\n\n\n\nIf none of the above, then what?\n\n\n\n\n\n(Finally) A positive vision for science after p-values.\n\n\n\n\n\n\nJul 19, 2023\n\n\nDaniel Saunders\n\n\n\n\n\n\n  \n\n\n\n\nMake Smart Choices, Use Multi-Level Models\n\n\n\n\n\nThe truth is out there. Only the multi-level model can find it.\n\n\n\n\n\n\nDec 22, 2022\n\n\nDaniel Saunders\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a principal data scientist at PyMC Labs, a consulting firm that specializes in building Bayesian models to handle business problems. The firm grew out of the open source project, PyMC, a truly wonderful piece of software.\nIn the recent past, I was a PhD student at the University of British Columbia in the philosophy program. I worked on topics in evolutionary game theory, philosophy of science, and cultural evolution. You can find my published work here.\nI occasionally blog about related topics on this website."
  },
  {
    "objectID": "posts/a-positive-constrained-non-centered-prior-that-sparks-joy/index.html",
    "href": "posts/a-positive-constrained-non-centered-prior-that-sparks-joy/index.html",
    "title": "A positive constrained non-centered prior that sparks joy",
    "section": "",
    "text": "with pm.Model() as m0:\n    sigma = pm.Gamma('sigma',mu=1.0,sigma=1.0)\n    phi = pm.LogNormal('phi',mu=1.0,sigma=sigma)\n    \ncompiled_model = nutpie.compile_pymc_model(m0)\nt0 = nutpie.sample(compiled_model,chains=4,tune=1000,draws=1000,progress_bar=False);"
  },
  {
    "objectID": "posts/a-positive-constrained-non-centered-prior-that-sparks-joy/index.html#they-can-funnel-just-like-hierarchical-normals",
    "href": "posts/a-positive-constrained-non-centered-prior-that-sparks-joy/index.html#they-can-funnel-just-like-hierarchical-normals",
    "title": "A positive constrained non-centered prior that sparks joy",
    "section": "",
    "text": "with pm.Model() as m0:\n    sigma = pm.Gamma('sigma',mu=1.0,sigma=1.0)\n    phi = pm.LogNormal('phi',mu=1.0,sigma=sigma)\n    \ncompiled_model = nutpie.compile_pymc_model(m0)\nt0 = nutpie.sample(compiled_model,chains=4,tune=1000,draws=1000,progress_bar=False);"
  },
  {
    "objectID": "posts/a-positive-constrained-non-centered-prior-that-sparks-joy/index.html#parameterizing-in-log-space-makes-it-hard-to-control-the-mean-and-variance-of-your-distribution",
    "href": "posts/a-positive-constrained-non-centered-prior-that-sparks-joy/index.html#parameterizing-in-log-space-makes-it-hard-to-control-the-mean-and-variance-of-your-distribution",
    "title": "A positive constrained non-centered prior that sparks joy",
    "section": "2. Parameterizing in log space makes it hard to control the mean and variance of your distribution",
    "text": "2. Parameterizing in log space makes it hard to control the mean and variance of your distribution\nOne strategy is to just work in log space. There, you can build a normal model that is unconstrained. This gives you access to familiar reparameterization techniques like non-centering. Then you exponential transform everything to enforce the positivity constraint.\n\nmu = 1\n\nwith pm.Model() as m1:\n    sigma = pm.Gamma('sigma',mu=1,sigma=1)\n\n    # non-centered trick\n    log_phi_z = pm.Normal(\"phi_offset\",mu=0,sigma=1) \n    log_phi = mu + log_phi_z * sigma\n\n    # make it positive\n    phi = pm.Deterministic(\"phi\",pm.math.exp(log_phi))\n    \ncompiled_model = nutpie.compile_pymc_model(m1)\nt1 = nutpie.sample(compiled_model,chains=4,tune=1000,draws=1000,progress_bar=False)\n\n\n\n\n\n\nThe problem is that we quickly lose control over the mean and variance. The exponential transform introduces a very long tail behaviour which blows the distribution up. Check out that mean and standard deviation:\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nphi\n1400.664\n75282.985\n0.0\n17.085\n1188.202\n37115.777\n2589.0\n2713.0\n1.0"
  },
  {
    "objectID": "posts/a-positive-constrained-non-centered-prior-that-sparks-joy/index.html#footnotes",
    "href": "posts/a-positive-constrained-non-centered-prior-that-sparks-joy/index.html#footnotes",
    "title": "A positive constrained non-centered prior that sparks joy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFarmed random seeds to get synthetic data that was easy + cranked the target_accept way up, both practices I’m not proud of.↩︎"
  },
  {
    "objectID": "posts/geometric-intuition-mmm/index.html",
    "href": "posts/geometric-intuition-mmm/index.html",
    "title": "Geometric intuition for media mix models",
    "section": "",
    "text": "MMMs are just fancy linear regressions.\nAt my work, it is a common refrain. It is meant as a sarcastic reminder - don’t go crazy, don’t convince yourself you are doing something more complicated than you are. This is all still pretty ordinary statistics. I think it undersells some of the more delicate aspects of media mix models (MMMs). MMMs are really non-linear regressions. The intuitions that data scientists develop by studying the theory of linear statistical models can be misleading. MMMs are ungodly creatures. They follow their own set of rules. In this post, I want to draw a few sketches to give you some new intuitions about the behaviour of MMMs.\nIf you have two inputs that you want to add together and pass through a linear function, it doesn’t matter what order you do things in. Add and then pass; pass and then add. Both yield the same result.\n\\[f(a+b) = f(a) + f(b)\\]\nSaturation functions are non-linear. It’s easy to see why. Suppose the function saturates at \\(0.5\\). If you pass two large quantities that both return something close to \\(0.5\\), then adding them up will be \\(\\approx 1\\). If, instead, you add the two large quantities and then pass to the saturation function, you get back \\(\\approx 0.5\\).\nA common choice of saturation function is the tanh.\n\\[ b * \\text{tanh}(\\frac{x}{bc})\\]\n\n\\(b\\) represents the saturation point. How many sales can you ever achieve by advertising on this channel.\n\\(c\\) represents the customer acquisition cost. This roughly translates to “how much money do you need to spend on this channel to acquire your first customer?”\n\nThe plan for the rest of this post is to explore how we estimate the parameters of a saturation function. We’ll look at small simulated data experiments - cases where we know the true parameters - and the corresponding likelihood surface. These will be extremely stripped down models. Nothing else going on but a saturation curve. We’ll see that a lot of surprising consequences arise from the geometry of the likelihood surface.\n\nMMMs aren’t just about saturation functions. What about the adstock?\nFor sure, but adstock is a linear function. If you smear two small quantities out over time and then add them together, it’s the same as smearing one big quantity. All the really tricky stuff arises from the non-linearity introduced by saturation functions.\n\n\nMMMs will have correlated parameter estimates, even under ideal conditions.\nLet’s suppose that the true, data-generating process is a tanh function with parameters:\n\n\\(b = 0.45\\)\n\\(c = 0.8\\)\n\nIf I didn’t know those parameters values but I searched a 2D grid of parameter values and assessed the log-likelihood at each point for the simulated dataset shown below, I would end up with a likelihood surface. Likelihood surfaces are nice to study because they play a controlling role in the inferences regardless of whether the MMM is Bayesian or not.\n\n\n\n\n\nFor the tanh function, the likelihood surface is banana-shaped. There is a region of approximately equal likelihood defined by combinations of \\(b\\) and \\(c\\) values. If your marketing channel is very effective at acquiring customers but saturates quickly, that explains the data. But if the marketing channel saturates slowly and is more expensive, that model also does a pretty good job.\nThe equivalence between pairs of parameter values is really not so bad on this particular example. The peak of the likelihood surface is very close to the true parameter values. We are in the good case though. We have plenty of data. It is nicely distributed between the high and low parts of the saturation curve. In other examples (we’ll explore in the next section), the equivalence can make it really difficult to discover the true parameter values of some channels.\nThe correlations between parameters are not limited to this parameterization of the tanh function. All of them will have this structure because the saturation point and the customer acquisition cost always interact. The resulting surface may not be banana-shaped and it may not parameterized in terms customer acquisition costs. But the lessons we learn in this post will apply no matter the choice of curve. Here is, for example, the Michaelis-Menten curve:\n\n\n\n\n\nThis is also a region of similar likelihood across pairs of parameters values.\n\n\nIt’s not about how much data you have but how that data is distributed across the saturation curve.\nLet’s suppose that your marketing department has been consistently and historically under-spending on a certain channel. At some point, popular wisdom took hold and the no one thinks cable tv ads are very effective. So the marketing team throws a few bucks toward cable each year and otherwise focuses on digital. Your observed data and resulting likelihood surface would look like:\n\n\n\n\n\nIt’s the same basic banana-shape but a lot stretching to the right. This indicates that the data is not terribly informative for the parameter value of the saturation point. How could it be? If you have never spent close to the saturation point, you will never find out when diminishing returns kicks in. This data can indicate a window of parameters that are plausible but it will never, in practice, converge to a single point. Below, I’ve highlighted a region of the surface where the value is within 1.5 logs of the global max.\n\n\n\n\n\nThis has a few big consequences for data science teams.\n\nCollecting more data may not help you if the data falls on the same place along the saturation curve. At this point, we have 20 observations. If you could persuade the marketing team to spend more money on cable for even one or two days, that would provide more information than doubling the size of the dataset.\nMMMs cannot confidently recommend things you have never tried. A marketing department might be curious whether they should spend more on cable. Given the evidence at hand, it is hard to say. The saturation point might be quite close to our current spending level (0.4) but it also might be very far away. At the very least, the parameter estimates we’d obtain would indicate that it is at least worth it to try to spend more. MMMs should be pitched as systematic summaries of the knowledge your company has acquired rather than as prophetic orbs.\n\nSimilar lessons apply for channels that the marketing team has historically overspent on.\n\n\n\n\n\nIf all our spending is near the saturation point, then we’ll have a pretty good idea of what that point is. But we won’t know how we got there: a fast saturation channel that spikes quickly and levels off or a smooth, gradual incline.\nThe region of nearly identical likelihood is just as bad for overspending. I’ve highlighted all the points that fall with in half a log of a max. These long tails of nearly equivalent likelihood can be quite problematic for summarizing the results of our model. In linear regression, the curvature around the maximum likelihood point is symmetrical. Here that’s not the case. The curvature is heavily asymmetrical. So if you are estimating posterior distributions, the mean of that posterior will not be particularly close to the max. The mean would be somewhere in the middle of that yellow band.\nReporting a mean like that to the marketing team could be quite misleading. It suggests the channel is very cheap. Maybe 50 cents of marketing material to convert the first customer. You don’t have a ton of options. Giving them a full distribution isn’t that helpful either. The distribution will say “customers could be free, very cheap or pretty expensive.” I suspect you’ll get questions about how we can trust models that are so uncertain and so on. There is a better way though - that is statistically defensible, improves the stability of estimators and samplers, and will align with the marketing departments expectations.\n\n\nGentle priors, gentle penalties\nWe should appreciate that the cases where we overspend or underspend are probably the typical case. Marketers are managing finite budgets and put themselves at considerable risk if they suddenly start spending a lot more on cable tv ads “just to try it.” Instead, in my experience, marketing teams tend to follow a tradition built out of some trial and error, some folklore and some past marketing research. This means that industry datasets tend to have underexplored the saturation curve.\nWhile our data cannot distinguish between some parameter values, our powerful minds can. Some of these parameter values are obviously implausible. Customers aren’t free. You cannot regularly persuade more than one person to buy something with a single impression. The folk wisdom of business suggests that marketing drives 5%-20% of sales. The saturation point cannot be larger than the entire potential customer base of your product. Statements like this allow us to slowly narrow the range of parameter values. Customer acquisition parameters can only go so low. Saturation points can only go so high.\nWe’d like to encode that information into the model. But we want to be as gentle as possible. The data should still do the talking. It’s just that a little bit of extra info can go a long way.\nConsider the case of the customer acquisition cost. An appropriate prior might be the inverse Gamma distribution.\n\n\n\n\n\nThis prior trims off the super low acquisition costs and allows you to pick a best guess. But it has a nice long tail so it is consistent the scenario where the marketing channels are very ineffective. If the data drives the estimates up there, the model will listen.\nHere is what the log likelihood surface looks like if we add the prior. The region of near equivalence begins to become more symmetrical. The peak of the surface moves away from the super low, implausible parameter values. In this way, priors serve as guardrails - in the absence of data-driven information, the prior can control the behaviour of the model. However, the prior clearly isn’t overpowering the data. The structure of the likelihood surface is almost exactly as it was before we added the prior.\n\n\n\n\n\nA similar lesson could be extended for the underspend case. Pruning very large saturation points will have the same effect. In this case, a Gamma distribution might be a nice choice because it has thin tails when sigma is small and doesn’t eliminate small \\(b\\) values.\nThere is no need to wait until you discover a channel has over spend or under spend. In practice, you won’t know when you are in one regime or another. Develop some gentle, default priors in collaboration with the marketing team and apply them to all channels. In the good cases, it won’t hurt. In the bad cases, it can really help.\n\nThis is all Bayesian stuff, how does this apply to frequentist or ML frameworks?\nI’ll be honest. I don’t really know anything about frequentism. But I’m told on good authority that you all use penalties and constraints to decrease the variance of estimators. If you can accomplish the same thing I sketched above with those tools, great. If you cannot, I’m concerned for you and recommend trying out Bayesian tools, at least for MMMs.\n\n\n\nTake home lessons\nMMMs are not like linear regressions in a few key places.\n\nThe precision of your parameter estimates will not necessarily increase with the size of the data set. What really matters is how well the data is distributed across the saturation curve.\nThey tend to be uncertain about the effects of marketing strategies you have never tried.\nPoint estimates can be fairly misleading without care in how they are communicated.\nModest prior constraints are often plausible and helpful in making posteriors easier to communicate and summarize.\n\nI won’t sketch these ideas out here. But in future blog posts, I also want to explore a couple of other consequences:\n\nHamiltonian Monte Carlo techniques should use dense mass matrix adaptations on MMMs.\nMMMs can have multi-modal likelihood surfaces once complexity of the model increases even a little bit."
  },
  {
    "objectID": "posts/if-none-of-the-above-then-what/if-none-of-the-above-then-what.html",
    "href": "posts/if-none-of-the-above-then-what/if-none-of-the-above-then-what.html",
    "title": "If none of the above, then what?",
    "section": "",
    "text": "Yesterday, Richard McElreath had a lovely blog post on why the Bayes factor isn’t the cure to what ails us. No p-values, no confidence intervals, either. No metric of any kind can replace the need for clear model-first thinking as the justification for scientific claims. A common reaction to Richard’s post is to ask the obvious question, “if none of the above, then what?” The question underscores just how difficult it is to imagine another kind of science.\nA casual acquaintance with the p-value wars of the last decade leaves one with the impression that we are searching for some metric which we can stamp on a scientific paper to let everyone know we did a good job. If not the p-value, then surely something else: a cross-validation score, a low AIC score, a big effect size, a big likelihood ratio. Maybe a good paper is the one that has (N = 1,000,000,000) in the abstract! There is a pretense to these conversations: if we just tweak our metric correctly, we can also tweak the publication pipeline and stabilize science. Bayesians in Richard’s camp are asking a provocative question - what if the pretense is wrong? How could we justify a scientific claim in the absence of those metrics?\nIt turns out the answer is not mysticism. A humble cycle of model-building and fitting is usually all we need. The metrics listed above have their proper place. It’s just that they are rarely what is called for. A model-first approach makes it obvious when they are useful and how to deploy them. The philosophical arguments for this approach have been spelled out before (Gelman and Shalizi 2013). But it is still quite hard to imagine what it looks like from these abstract methodological descriptions. The rest of this post provides a compact and easy to follow example of the full model-first workflow. The promise is that, by the end, we won’t have needed a metric to assess our model and you won’t miss them. We’ll learn something important without ever reaching for them.\nWe are going to study a model of technological innovation. The central claim is that big populations innovate more than small populations. We’ll build up the model from basic intuitions and evaluate it against a historical dataset of island societies in Oceania. It will feel a bit too easy and that’s the point. Once we carefully model our problem, there is no need to argue about p-values, Bayes factors, confidence intervals or the like."
  },
  {
    "objectID": "posts/if-none-of-the-above-then-what/if-none-of-the-above-then-what.html#footnotes",
    "href": "posts/if-none-of-the-above-then-what/if-none-of-the-above-then-what.html#footnotes",
    "title": "If none of the above, then what?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor many technologies, there will not be some continuous improvement that can be equated directly with a skill level. Instead, there is some further function that maps skillfulness onto discrete stages of improvement in a gadget or even the variety of gadgets a person can produce. Henrich sets this complexity aside to illustrate a general relationship that should hold approximately in the complex case and we’ll do the same I think this is a pretty big simplification. Simplifications like this serve as an opportunity for future work, fleshing out the theory and seeing what is plausible in light of empirical evidence. To be fair, people have challenged Henrich on this one (Vaesen et al. 2016).↩︎\nIf you are wondering whether we can reliably measure the complexity of a tool based on ethnographic reports written during the period of European colonization, you are not alone (Jerven 2011). But the part of this case study that is interesting for our purposes does not concern data quality. I will assume these measures are appropriate for the sake of the larger argument.↩︎\nWe don’t have to use a differential equation solver. We could just integrate the differential equation to get an analytical expression for the number of tools as function of \\(a,b,n,t\\). It would be faster and more accurate too. But I had already worked out the differential equation approach by the time I realized it was an easy integration problem so here’s what we get. Still nice to know the fitting differential equations ain’t too hard.↩︎"
  }
]